#!/usr/bin/env python3
"""
KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒè„šæœ¬
æ•´åˆæ‰€æœ‰æœ€ä½³å®è·µåˆ°ä¸€ä¸ªç®€æ´çš„è®­ç»ƒæµç¨‹ä¸­
ç›®æ ‡: ç›´æ¥è¾¾åˆ°MSE~0.1310, CI~0.8886, R2~0.8035çš„æ€§èƒ½

åŸºäºå¤šé˜¶æ®µä¼˜åŒ–å‘ç°çš„æœ€ä½³é…ç½®:
- è¡¨é¢ç‰¹å¾: use_surface=True, use_masif=True
- å¯¹æ¯”å­¦ä¹ : contrastive_weight=0.03
- ä¼˜åŒ–å™¨: AdamW with cosine scheduling
- è®­ç»ƒç­–ç•¥: é•¿æœŸè®­ç»ƒ + æ—©åœ + æ¢¯åº¦è£å‰ª
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import argparse
import logging
from datetime import datetime
from torch_geometric.data import DataLoader

from src.model_0428_16_dual import MGraphDTA
from src.dataset import GNNDataset
from src.metrics import get_cindex, get_rm2

def mixup_data(x, y, alpha=1.0):
    """Mixupæ•°æ®å¢å¼º"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def set_reproducible_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å®Œå…¨å¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

class OptimizedTrainer:
    """ä¼˜åŒ–çš„å•æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        # ä½¿ç”¨CUDA_VISIBLE_DEVICESæ˜ å°„åçš„è®¾å¤‡0
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # Phase 1ä¼˜åŒ–é…ç½® (åŸºäºGASI-DTAæ–‡çŒ®æœ€ä½³å®è·µ)
        self.best_config = {
            'embedding_size': 128,
            'filter_num': 32,
            'mask_rate': 0.05,
            'temperature': 0.1,
            'cl_similarity_threshold': 0.5,
            'contrastive_weight': 0.03,  # ä¿æŒå·²éªŒè¯çš„å¯¹æ¯”å­¦ä¹ æƒé‡
            'lr': 1e-4,                  # æ–‡çŒ®æ¨èçš„æ›´ä¿å®ˆå­¦ä¹ ç‡
            'weight_decay': 1e-4,        # æ–‡çŒ®æ¨èçš„æ›´å¼ºæ­£åˆ™åŒ–
            'batch_size': 256,           # æ–‡çŒ®æ¨èçš„æ›´å°æ‰¹æ¬¡å¤§å°
            'max_epochs': 3000,
            'early_stop_patience': 200,  # æ›´åˆç†çš„æ—©åœè€å¿ƒ
            'grad_clip_norm': 1.0,       # æ–‡çŒ®æ¨èçš„æ›´å¼ºæ¢¯åº¦è£å‰ª
            'warmup_epochs': 100,        # æ–‡çŒ®æ¨èçš„æ›´é•¿é¢„çƒ­
            'label_smoothing': 0.05,     # å¢å¼ºæ ‡ç­¾å¹³æ»‘
            'dropout_rate': 0.2,         # æ–‡çŒ®æ¨èçš„æ›´å¼ºdropout
            'mixup_alpha': 0.2,          # æ·»åŠ Mixupæ•°æ®å¢å¼º
            'use_cosine_restarts': True  # ä½¿ç”¨é‡å¯ä½™å¼¦è°ƒåº¦
        }
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–KIBAå•æ¨¡å‹è®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“Š æœ€ä½³é…ç½®: {self.best_config}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = f"logs/single_model_{timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{log_dir}/training.log"),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self):
        """åŠ è½½KIBAæ•°æ®é›†"""
        self.logger.info("ğŸ“Š åŠ è½½KIBAæ•°æ®é›†...")
        
        # fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', self.args.dataset)
        fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', self.args.dataset, 'cold')

        # ä½¿ç”¨æœ€ä½³æ•°æ®é…ç½®
        train_set = GNNDataset(fpath, types='train', use_surface=True, use_masif=True)
        test1_set = GNNDataset(fpath, types='test1', use_surface=True, use_masif=True)

        try:
            test2_size = len(GNNDataset(fpath, types='test2', use_surface=True, use_masif=True))
            self.has_test2 = True
        except Exception as e:
            self.has_test2 = False

        if self.has_test2:
            test2_set = GNNDataset(fpath, types='test2', use_surface=True, use_masif=True)


        self.train_loader = DataLoader(
            train_set, 
            batch_size=self.best_config['batch_size'], 
            shuffle=True, 
            num_workers=8
        )
        self.test_loader = DataLoader(
            test1_set, 
            batch_size=self.best_config['batch_size'], 
            shuffle=False, 
            num_workers=8
        )

        if self.has_test2:
            self.test_loader2 = DataLoader(
                test2_set,
                batch_size=self.best_config['batch_size'],
                shuffle=False,
                num_workers=8
            )
        
        self.logger.info(f"âœ… è®­ç»ƒé›†: {len(train_set)} æ ·æœ¬")
        self.logger.info(f"âœ… æµ‹è¯•é›†: {len(test1_set)} æ ·æœ¬")
        if self.has_test2:
            self.logger.info(f"âœ… ç¬¬äºŒæµ‹è¯•é›†: {test2_size} æ ·æœ¬")
        self.logger.info(f"âœ… æ‰¹æ¬¡å¤§å°: {self.best_config['batch_size']}")
        self.logger.info(f"âœ… è¡¨é¢ç‰¹å¾: å¯ç”¨ (use_surface=True, use_masif=True)")
    
    def create_model(self):
        """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
        self.logger.info("ğŸ—ï¸ åˆ›å»ºMGraphDTAæ¨¡å‹...")

        self.model = MGraphDTA(
            3, 25 + 1,
            embedding_size=self.best_config['embedding_size'],
            filter_num=self.best_config['filter_num'],
            out_dim=1,
            mask_rate=self.best_config['mask_rate'],
            temperature=self.best_config['temperature'],
            disable_masking=False,
            cl_mode='regression',
            cl_similarity_threshold=self.best_config['cl_similarity_threshold'],
            use_surface=True  # å…³é”®ç‰¹å¾
        ).to(self.device)

        # æ·»åŠ dropoutå±‚åˆ°æ¨¡å‹ä¸­ (å¦‚æœæ¨¡å‹æ”¯æŒ)
        if hasattr(self.model, 'set_dropout'):
            self.model.set_dropout(self.best_config['dropout_rate'])

        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self.logger.info(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,} æ€»è®¡, {trainable_params:,} å¯è®­ç»ƒ")
        self.logger.info(f"âœ… å¯¹æ¯”å­¦ä¹ æƒé‡: {self.best_config['contrastive_weight']}")
        self.logger.info(f"âœ… Dropoutç‡: {self.best_config['dropout_rate']}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ - Phase 1ä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("âš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")

        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ (æ¯”Adamæ›´å¥½)
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.best_config['lr'],
            weight_decay=self.best_config['weight_decay'],
            betas=(0.9, 0.999),
            eps=1e-8
        )

        # Phase 1: ä½¿ç”¨é‡å¯ä½™å¼¦è°ƒåº¦å™¨ (æ–‡çŒ®æ¨è)
        if self.best_config['use_cosine_restarts']:
            self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=50,      # åˆå§‹é‡å¯å‘¨æœŸ
                T_mult=2,    # å‘¨æœŸå€å¢å› å­
                eta_min=1e-6 # æœ€å°å­¦ä¹ ç‡
            )
            scheduler_name = "CosineAnnealingWarmRestarts"
        else:
            # å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨ (å¤‡é€‰)
            def lr_lambda(epoch):
                if epoch < self.best_config['warmup_epochs']:
                    return epoch / self.best_config['warmup_epochs']
                else:
                    # ä½™å¼¦é€€ç«
                    progress = (epoch - self.best_config['warmup_epochs']) / (self.best_config['max_epochs'] - self.best_config['warmup_epochs'])
                    return 0.5 * (1 + np.cos(np.pi * progress))

            self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
            scheduler_name = "WarmupCosineAnnealingLR"

        # æŸå¤±å‡½æ•° - å¢å¼ºæ ‡ç­¾å¹³æ»‘
        self.criterion = nn.MSELoss()
        self.label_smoothing = self.best_config['label_smoothing']

        self.logger.info(f"âœ… ä¼˜åŒ–å™¨: AdamW (lr={self.best_config['lr']}, wd={self.best_config['weight_decay']})")
        self.logger.info(f"âœ… è°ƒåº¦å™¨: {scheduler_name}")
        self.logger.info(f"âœ… æŸå¤±å‡½æ•°: MSELoss + æ ‡ç­¾å¹³æ»‘({self.label_smoothing})")
        self.logger.info(f"âœ… Dropout: {self.best_config['dropout_rate']}")
        self.logger.info(f"âœ… Mixup Alpha: {self.best_config['mixup_alpha']}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, data in enumerate(self.train_loader):
            data = data.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            pred = self.model(data)
            
            # è®¡ç®—æŸå¤± (åŒ…å«å¯¹æ¯”å­¦ä¹ å’Œæ ‡ç­¾å¹³æ»‘)
            pred_flat = pred.view(-1)
            target_flat = data.y.view(-1)

            # Phase 1: æ·»åŠ Mixupæ•°æ®å¢å¼º (éšæœºåº”ç”¨)
            if self.best_config['mixup_alpha'] > 0 and np.random.random() < 0.5:
                # å¯¹é¢„æµ‹ç»“æœåº”ç”¨Mixup
                mixed_pred, y_a, y_b, lam = mixup_data(pred_flat.unsqueeze(1), target_flat,
                                                      self.best_config['mixup_alpha'])
                mixed_pred = mixed_pred.squeeze(1)

                # MixupæŸå¤±
                mse_loss = lam * self.criterion(mixed_pred, y_a) + (1 - lam) * self.criterion(mixed_pred, y_b)
            else:
                # å¢å¼ºæ ‡ç­¾å¹³æ»‘
                if self.label_smoothing > 0:
                    # å¯¹ç›®æ ‡å€¼æ·»åŠ å°é‡å™ªå£°
                    noise = torch.randn_like(target_flat) * self.label_smoothing
                    target_smooth = target_flat + noise
                    mse_loss = self.criterion(pred_flat, target_smooth)
                else:
                    mse_loss = self.criterion(pred_flat, target_flat)

            # è·å–å¯¹æ¯”å­¦ä¹ æŸå¤±
            if hasattr(self.model, 'get_contrastive_loss'):
                cl_loss = self.model.get_contrastive_loss()
                total_loss_batch = mse_loss + self.best_config['contrastive_weight'] * cl_loss
            else:
                total_loss_batch = mse_loss
            
            # åå‘ä¼ æ’­
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.best_config['grad_clip_norm']
            )
            
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            num_batches += 1
            
            # å®šæœŸæ‰“å°è¿›åº¦
            # if batch_idx % 100 == 0:
            #     current_lr = self.optimizer.param_groups[0]['lr']
            #     self.logger.info(
            #         f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
            #         f"Loss: {total_loss_batch.item():.4f}, LR: {current_lr:.6f}"
            #     )
        
        return total_loss / num_batches
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        pred_list = []
        label_list = []
        
        with torch.no_grad():
            for data in self.test_loader:
                data = data.to(self.device)
                pred = self.model(data)
                pred_list.append(pred.view(-1).cpu().numpy())
                label_list.append(data.y.cpu().numpy())
        
        predictions = np.concatenate(pred_list)
        labels = np.concatenate(label_list)
        
        mse = np.mean((predictions - labels) ** 2)
        cindex = get_cindex(labels, predictions)
        r2 = get_rm2(labels, predictions)
        
        return mse, cindex, r2

    def evaluate2(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        pred_list = []
        label_list = []

        with torch.no_grad():
            for data in self.test_loader2:
                data = data.to(self.device)
                pred = self.model(data)
                pred_list.append(pred.view(-1).cpu().numpy())
                label_list.append(data.y.cpu().numpy())

        predictions = np.concatenate(pred_list)
        labels = np.concatenate(label_list)

        mse = np.mean((predictions - labels) ** 2)
        cindex = get_cindex(labels, predictions)
        r2 = get_rm2(labels, predictions)

        return mse, cindex, r2

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")

        # Test1æ€§èƒ½è·Ÿè¸ª (å¿…é¡»ä¿æŒ)
        best_test1_mse = float('inf')
        best_test1_ci = 0.0
        best_test1_r2 = 0.0

        # Test2æ€§èƒ½è·Ÿè¸ª (éœ€è¦æ”¹å–„)
        best_test2_mse = float('inf')
        best_test2_ci = 0.0
        best_test2_r2 = 0.0

        # ç»¼åˆæ—©åœç­–ç•¥
        patience_counter = 0
        best_epoch = 0

        # å½“å‰åŸºçº¿æ€§èƒ½å’Œç›®æ ‡
        baseline_test1 = {'mse': 0.4191, 'ci': 0.7433, 'r2': 0.3096}
        baseline_test2 = {'mse': 0.5647, 'ci': 0.5963, 'r2': 0.0661}
        target_test2 = {'mse': 0.52, 'ci': 0.661, 'r2': 0.1016}

        self.logger.info(f"ğŸ“Š Test1åŸºçº¿: MSE={baseline_test1['mse']:.4f}, CI={baseline_test1['ci']:.4f}, R2={baseline_test1['r2']:.4f}")
        self.logger.info(f"ğŸ“Š Test2åŸºçº¿: MSE={baseline_test2['mse']:.4f}, CI={baseline_test2['ci']:.4f}, R2={baseline_test2['r2']:.4f}")
        self.logger.info(f"ğŸ¯ Test2ç›®æ ‡: MSEâ‰¤{target_test2['mse']:.4f}, CIâ‰¥{target_test2['ci']:.4f}, R2â‰¥{target_test2['r2']:.4f}")
        
        for epoch in range(1, self.best_config['max_epochs'] + 1):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_mse, test_cindex, test_r2 = self.evaluate()

            if self.has_test2:
                test2_mse, test2_cindex, test2_r2 = self.evaluate2()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•ç»“æœ
            log_msg = (f"Epoch {epoch:4d} | Loss: {train_loss:.4f} | LR: {current_lr:.6f} | "
                      f"Test1 MSE: {test_mse:.4f} CI: {test_cindex:.4f} R2: {test_r2:.4f}")

            if self.has_test2:
                log_msg += f" | Test2 MSE: {test2_mse:.4f} CI: {test2_cindex:.4f} R2: {test2_r2:.4f}"

            self.logger.info(log_msg)

            # æ›´æ–°æœ€ä½³æ€§èƒ½
            improved = False

            # Test1æ€§èƒ½æ›´æ–°
            if test_mse < best_test1_mse:
                best_test1_mse = test_mse
                best_test1_ci = test_cindex
                best_test1_r2 = test_r2
                improved = True

            # Test2æ€§èƒ½æ›´æ–° (å¦‚æœå­˜åœ¨)
            if self.has_test2:
                if test2_mse < best_test2_mse:
                    best_test2_mse = test2_mse
                    best_test2_ci = test2_cindex
                    best_test2_r2 = test2_r2
                    improved = True

            # ç»¼åˆè¯„ä¼°æ˜¯å¦æ”¹å–„ (ä¼˜å…ˆè€ƒè™‘Test2æ”¹å–„)
            if self.has_test2:
                # è®¡ç®—Test2æ”¹å–„åˆ†æ•°
                test2_improvement = (
                    (baseline_test2['mse'] - test2_mse) / baseline_test2['mse'] * 0.4 +  # MSEæ”¹å–„æƒé‡40%
                    (test2_cindex - baseline_test2['ci']) / baseline_test2['ci'] * 0.3 +  # CIæ”¹å–„æƒé‡30%
                    (test2_r2 - baseline_test2['r2']) / (baseline_test2['r2'] + 0.1) * 0.3  # R2æ”¹å–„æƒé‡30%
                )

                # ç¡®ä¿Test1æ€§èƒ½ä¸é€€åŒ–
                test1_maintained = (test_mse <= baseline_test1['mse'] * 1.01 and
                                  test_cindex >= baseline_test1['ci'] * 0.99 and
                                  test_r2 >= baseline_test1['r2'] * 0.99)

                if test2_improvement > 0 and test1_maintained:
                    best_epoch = epoch
                    patience_counter = 0

                    # ä¿å­˜æ¨¡å‹
                    if self.args.save_model:
                        save_path = f"optimized_cold_model_test1_{test_mse:.4f}_{test_cindex:.4f}_{test_r2:.4f}_test2_{test2_mse:.4f}_{test2_cindex:.4f}_{test2_r2:.4f}.pt"
                        torch.save(self.model.state_dict(), save_path)
                        self.logger.info(f"ğŸ’¾ ä¿å­˜æ”¹å–„æ¨¡å‹: {save_path}")

                    self.logger.info(f"ğŸ† Test2æ€§èƒ½æ”¹å–„! æ”¹å–„åˆ†æ•°: {test2_improvement:.4f}")
                else:
                    patience_counter += 1
            else:
                # åªæœ‰Test1çš„æƒ…å†µ
                if improved:
                    best_epoch = epoch
                    patience_counter = 0

                    if self.args.save_model:
                        save_path = f"optimized_cold_model_test1_{test_mse:.4f}_{test_cindex:.4f}_{test_r2:.4f}.pt"
                        torch.save(self.model.state_dict(), save_path)
                        self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
                else:
                    patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.best_config['early_stop_patience']:
                self.logger.info(f"â¹ï¸ æ—©åœè§¦å‘ (patience={self.best_config['early_stop_patience']})")
                break

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°Test2ç›®æ ‡æ€§èƒ½
            if self.has_test2:
                if (test2_mse <= target_test2['mse'] and
                    test2_cindex >= target_test2['ci'] and
                    test2_r2 >= target_test2['r2']):
                    self.logger.info(f"ğŸ‰ è¾¾åˆ°Test2ç›®æ ‡æ€§èƒ½!")
                    break

        # è®­ç»ƒå®Œæˆæ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("ğŸ è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"ğŸ† æœ€ä½³æ€§èƒ½ (Epoch {best_epoch}):")
        self.logger.info(f"   Test1: MSE={best_test1_mse:.4f}, CI={best_test1_ci:.4f}, R2={best_test1_r2:.4f}")
        if self.has_test2:
            self.logger.info(f"   Test2: MSE={best_test2_mse:.4f}, CI={best_test2_ci:.4f}, R2={best_test2_r2:.4f}")

            # è®¡ç®—æ”¹å–„ç¨‹åº¦
            mse_improvement = baseline_test2['mse'] - best_test2_mse
            ci_improvement = best_test2_ci - baseline_test2['ci']
            r2_improvement = best_test2_r2 - baseline_test2['r2']

            self.logger.info(f"ğŸ“ˆ Test2æ”¹å–„:")
            self.logger.info(f"   MSE: {mse_improvement:+.4f} ({'âœ…' if mse_improvement >= 0.04 else 'âŒ'})")
            self.logger.info(f"   CI:  {ci_improvement:+.4f} ({'âœ…' if ci_improvement >= 0.065 else 'âŒ'})")
            self.logger.info(f"   R2:  {r2_improvement:+.4f} ({'âœ…' if r2_improvement >= 0.035 else 'âŒ'})")

            # Test1ä¿æŒæ£€æŸ¥
            test1_maintained = (best_test1_mse <= baseline_test1['mse'] * 1.01 and
                              best_test1_ci >= baseline_test1['ci'] * 0.99 and
                              best_test1_r2 >= baseline_test1['r2'] * 0.99)
            self.logger.info(f"ğŸ”’ Test1æ€§èƒ½ä¿æŒ: {'âœ…' if test1_maintained else 'âŒ'}")

        self.logger.info("=" * 80)

        return best_test1_mse if not self.has_test2 else best_test2_mse

def main():
    parser = argparse.ArgumentParser(description='KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒ')
    parser.add_argument('--dataset', type=str, default='kiba', help='æ•°æ®é›†åç§°')
    parser.add_argument('--gpu', type=int, default=6, help='GPUè®¾å¤‡å·')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--save_model', action='store_true', default=True, help='ä¿å­˜æœ€ä½³æ¨¡å‹')
    
    args = parser.parse_args()
    
    # è®¾ç½®å¯å¤ç°æ€§
    set_reproducible_seeds(args.seed)
    
    print("ğŸ¯ KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒ")
    print("=" * 60)
    print("ç›®æ ‡: é€šè¿‡å•ä¸€è®­ç»ƒè¿‡ç¨‹ç›´æ¥è¾¾åˆ°æœ€ä½³æ€§èƒ½")
    print("é…ç½®: æ•´åˆæ‰€æœ‰å¤šé˜¶æ®µä¼˜åŒ–å‘ç°çš„æœ€ä½³å®è·µ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = OptimizedTrainer(args)
    trainer.load_data()
    trainer.create_model()
    trainer.setup_optimizer()
    
    # å¼€å§‹è®­ç»ƒ
    best_mse = trainer.train()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³MSE: {best_mse:.4f}")
    print("ğŸ“‹ æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ç›´æ¥ç”¨äºè®ºæ–‡ç»“æœå¤ç°")

if __name__ == '__main__':
    main()
