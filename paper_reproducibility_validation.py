#!/usr/bin/env python3
"""
è®ºæ–‡å¯å¤ç°æ€§éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯è®ºæ–‡ä¸­æŠ¥å‘Šçš„KIBAä¼˜åŒ–ç»“æœ
ç¡®ä¿å…¶ä»–ç ”ç©¶è€…èƒ½å¤Ÿå¤ç°æˆ‘ä»¬çš„å…³é”®å‘ç°
"""

import os
import torch
import numpy as np
import random
import json
from datetime import datetime
from torch_geometric.data import DataLoader

from metrics import get_cindex, get_rm2
from dataset import *
from model_0428_16_dual import MGraphDTA

# ğŸ”’ å›ºå®šéšæœºç§å­ç¡®ä¿å®Œå…¨å¯å¤ç°
def set_reproducible_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å®Œå…¨å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ… è®¾ç½®éšæœºç§å­: {seed} (ç¡®ä¿å®Œå…¨å¯å¤ç°)")

class PaperResultsValidator:
    """è®ºæ–‡ç»“æœéªŒè¯å™¨"""
    
    def __init__(self, device='cpu'):
        self.device = torch.device(device)
        self.paper_results = {
            # è®ºæ–‡ä¸­æŠ¥å‘Šçš„å…³é”®ç»“æœ
            'baseline_training': {
                'epochs': 1365,
                'mse': 0.1330,
                'cindex': 0.8886,
                'r2': 0.7746,
                'description': 'é•¿æœŸè®­ç»ƒä¼˜åŒ–ç»“æœ'
            },
            'model_ensemble': {
                'models_used': 6,
                'mse': 0.1321,
                'cindex': 0.8891,
                'r2': 0.7805,
                'description': '6æ¨¡å‹é›†æˆç»“æœ'
            },
            'prediction_calibration': {
                'method': 'isotonic',
                'mse': 0.1310,
                'cindex': 0.8886,
                'r2': 0.8035,
                'description': 'Isotonicæ ¡å‡†åç»“æœ'
            },
            'advanced_ensemble': {
                'method': 'stacking',
                'mse': 0.1303,
                'cindex': 0.8883,
                'r2': 0.8053,
                'description': 'Stackingé›†æˆ+å¤šé˜¶æ®µæ ¡å‡†'
            }
        }
        
        self.tolerance = {
            'mse': 0.002,    # MSEå®¹å·® Â±0.002
            'cindex': 0.005, # CIå®¹å·® Â±0.005
            'r2': 0.01       # R2å®¹å·® Â±0.01
        }
    
    def validate_environment(self):
        """éªŒè¯è¿è¡Œç¯å¢ƒ"""
        print("ğŸ” éªŒè¯è¿è¡Œç¯å¢ƒ...")
        
        env_info = {
            'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'device': str(self.device),
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"   Pythonç‰ˆæœ¬: {env_info['python_version']}")
        print(f"   PyTorchç‰ˆæœ¬: {env_info['torch_version']}")
        print(f"   CUDAå¯ç”¨: {env_info['cuda_available']}")
        print(f"   ä½¿ç”¨è®¾å¤‡: {env_info['device']}")
        
        return env_info
    
    def load_best_models(self):
        """åŠ è½½è®ºæ–‡ä¸­ä½¿ç”¨çš„æœ€ä½³æ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½è®ºæ–‡ä¸­çš„æœ€ä½³æ¨¡å‹...")
        
        # è®ºæ–‡ä¸­ä½¿ç”¨çš„æœ€ä½³æ¨¡å‹è·¯å¾„
        model_paths = [
            'save/20250725_233313_kiba/model/epoch-1344, LR-0.000009, MSEloss-0.1232, cindex-0.8529, r2-0.7146, test1: [MSEloss-0.1328, cindex:0.8885, r2:0.7805].pt',
            'save/20250725_233313_kiba/model/epoch-1323, LR-0.000011, MSEloss-0.1231, cindex-0.8546, r2-0.7115, test1: [MSEloss-0.1329, cindex:0.8884, r2:0.7780].pt',
            'save/20250725_233313_kiba/model/epoch-1400, LR-0.000004, MSEloss-0.1223, cindex-0.8561, r2-0.7152, test1: [MSEloss-0.1327, cindex:0.8888, r2:0.7760].pt',
            'save/20250725_233313_kiba/model/epoch-1317, LR-0.000012, MSEloss-0.1233, cindex-0.8547, r2-0.7152, test1: [MSEloss-0.1331, cindex:0.8886, r2:0.7775].pt'
        ]
        
        models = []
        successful_loads = 0
        
        for i, path in enumerate(model_paths):
            if os.path.exists(path):
                try:
                    model = MGraphDTA(3, 25 + 1,
                                     embedding_size=128,
                                     filter_num=32,
                                     out_dim=1,
                                     mask_rate=0.05,
                                     temperature=0.1,
                                     disable_masking=False,
                                     cl_mode='regression',
                                     cl_similarity_threshold=0.5,
                                     use_surface=True).to(self.device)
                    
                    state_dict = torch.load(path, map_location=self.device)
                    model.load_state_dict(state_dict)
                    model.eval()
                    
                    models.append(model)
                    successful_loads += 1
                    print(f"   âœ… æ¨¡å‹ {i+1} åŠ è½½æˆåŠŸ")
                    
                except Exception as e:
                    print(f"   âŒ æ¨¡å‹ {i+1} åŠ è½½å¤±è´¥: {e}")
            else:
                print(f"   âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {os.path.basename(path)}")
        
        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {successful_loads}/{len(model_paths)} ä¸ªæ¨¡å‹")
        return models
    
    def validate_single_model_performance(self, model, model_name="single_model"):
        """éªŒè¯å•ä¸ªæ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ§ª éªŒè¯å•æ¨¡å‹æ€§èƒ½: {model_name}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        DATASET = 'kiba'
        fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', DATASET)
        test1_set = GNNDataset(fpath, types='test1', use_surface=True, use_masif=True)
        test1_loader = DataLoader(test1_set, batch_size=512, shuffle=False, num_workers=8)
        
        # ç”Ÿæˆé¢„æµ‹
        pred_list = []
        label_list = []
        
        with torch.no_grad():
            for data in test1_loader:
                data = data.to(self.device)
                pred = model(data)
                pred_list.append(pred.view(-1).cpu().numpy())
                label_list.append(data.y.cpu().numpy())
        
        predictions = np.concatenate(pred_list)
        labels = np.concatenate(label_list)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        mse = np.mean((predictions - labels) ** 2)
        cindex = get_cindex(labels, predictions)
        r2 = get_rm2(labels, predictions)
        
        result = {
            'mse': mse,
            'cindex': cindex,
            'r2': r2,
            'predictions': predictions,
            'labels': labels
        }
        
        print(f"   MSE: {mse:.4f}")
        print(f"   CI: {cindex:.4f}")
        print(f"   R2: {r2:.4f}")
        
        return result
    
    def validate_ensemble_performance(self, models):
        """éªŒè¯é›†æˆæ¨¡å‹æ€§èƒ½"""
        print("ğŸ§ª éªŒè¯é›†æˆæ¨¡å‹æ€§èƒ½...")
        
        if len(models) < 2:
            print("âŒ æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé›†æˆéªŒè¯")
            return None
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        DATASET = 'kiba'
        fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', DATASET)
        test1_set = GNNDataset(fpath, types='test1', use_surface=True, use_masif=True)
        test1_loader = DataLoader(test1_set, batch_size=512, shuffle=False, num_workers=8)
        
        # ç”Ÿæˆé›†æˆé¢„æµ‹
        all_predictions = []
        labels = None
        
        for i, model in enumerate(models):
            print(f"   ç”Ÿæˆæ¨¡å‹ {i+1} é¢„æµ‹...")
            
            batch_preds = []
            batch_labels = []
            
            with torch.no_grad():
                for data in test1_loader:
                    data = data.to(self.device)
                    pred = model(data)
                    batch_preds.append(pred.view(-1).cpu().numpy())
                    if labels is None:
                        batch_labels.append(data.y.cpu().numpy())
            
            all_predictions.append(np.concatenate(batch_preds))
            if labels is None:
                labels = np.concatenate(batch_labels)
        
        # ç­‰æƒé‡é›†æˆ
        ensemble_pred = np.mean(all_predictions, axis=0)
        
        # è®¡ç®—é›†æˆæ€§èƒ½
        mse = np.mean((ensemble_pred - labels) ** 2)
        cindex = get_cindex(labels, ensemble_pred)
        r2 = get_rm2(labels, ensemble_pred)
        
        result = {
            'mse': mse,
            'cindex': cindex,
            'r2': r2,
            'predictions': ensemble_pred,
            'labels': labels,
            'individual_predictions': all_predictions
        }
        
        print(f"   é›†æˆMSE: {mse:.4f}")
        print(f"   é›†æˆCI: {cindex:.4f}")
        print(f"   é›†æˆR2: {r2:.4f}")
        
        return result
    
    def compare_with_paper_results(self, actual_results, paper_key):
        """ä¸è®ºæ–‡ç»“æœå¯¹æ¯”"""
        paper_result = self.paper_results[paper_key]
        
        print(f"\nğŸ“Š ä¸è®ºæ–‡ç»“æœå¯¹æ¯” ({paper_result['description']}):")
        print("-" * 50)
        
        comparisons = {}
        all_within_tolerance = True
        
        for metric in ['mse', 'cindex', 'r2']:
            paper_value = paper_result[metric]
            actual_value = actual_results[metric]
            diff = abs(actual_value - paper_value)
            tolerance = self.tolerance[metric]
            within_tolerance = diff <= tolerance
            
            comparisons[metric] = {
                'paper': paper_value,
                'actual': actual_value,
                'difference': diff,
                'tolerance': tolerance,
                'within_tolerance': within_tolerance
            }
            
            status = "âœ… é€šè¿‡" if within_tolerance else "âŒ è¶…å‡ºå®¹å·®"
            print(f"{metric.upper():>6}: è®ºæ–‡={paper_value:.4f}, å®é™…={actual_value:.4f}, å·®å¼‚={diff:.4f}, {status}")
            
            if not within_tolerance:
                all_within_tolerance = False
        
        print("-" * 50)
        if all_within_tolerance:
            print("ğŸ‰ æ‰€æœ‰æŒ‡æ ‡éƒ½åœ¨å¯æ¥å—å®¹å·®èŒƒå›´å†…ï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†æŒ‡æ ‡è¶…å‡ºå®¹å·®èŒƒå›´")
        
        return comparisons, all_within_tolerance
    
    def generate_reproducibility_report(self, validation_results):
        """ç”Ÿæˆå¯å¤ç°æ€§æŠ¥å‘Š"""
        report = {
            'validation_timestamp': datetime.now().isoformat(),
            'environment': validation_results.get('environment', {}),
            'model_loading': validation_results.get('model_loading', {}),
            'performance_comparisons': validation_results.get('comparisons', {}),
            'overall_reproducibility': validation_results.get('overall_success', False),
            'recommendations': []
        }
        
        # æ·»åŠ å»ºè®®
        if report['overall_reproducibility']:
            report['recommendations'].append("âœ… ç»“æœå®Œå…¨å¯å¤ç°ï¼Œå¯ä»¥å®‰å…¨æäº¤è®ºæ–‡")
        else:
            report['recommendations'].append("âš ï¸ éƒ¨åˆ†ç»“æœå­˜åœ¨å·®å¼‚ï¼Œå»ºè®®æ£€æŸ¥ç¯å¢ƒé…ç½®")
            report['recommendations'].append("ğŸ’¡ ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­å’Œæ¨¡å‹æƒé‡")
            report['recommendations'].append("ğŸ”§ éªŒè¯æ•°æ®é¢„å¤„ç†æµç¨‹çš„ä¸€è‡´æ€§")
        
        return report

def main():
    print("ğŸ”¬ KIBAä¼˜åŒ–è®ºæ–‡å¯å¤ç°æ€§éªŒè¯")
    print("=" * 60)
    
    # è®¾ç½®å¯å¤ç°æ€§
    set_reproducible_seeds(42)
    
    # åˆå§‹åŒ–éªŒè¯å™¨
    device = 'cpu'  # ä½¿ç”¨CPUç¡®ä¿è·¨å¹³å°ä¸€è‡´æ€§
    validator = PaperResultsValidator(device=device)
    
    # éªŒè¯ç¯å¢ƒ
    env_info = validator.validate_environment()
    
    # åŠ è½½æ¨¡å‹
    models = validator.load_best_models()
    
    validation_results = {
        'environment': env_info,
        'model_loading': {'successful_models': len(models)},
        'comparisons': {},
        'overall_success': True
    }
    
    if len(models) > 0:
        # éªŒè¯å•æ¨¡å‹æ€§èƒ½
        single_result = validator.validate_single_model_performance(models[0], "best_single_model")
        
        # éªŒè¯é›†æˆæ€§èƒ½
        if len(models) >= 2:
            ensemble_result = validator.validate_ensemble_performance(models[:4])  # ä½¿ç”¨å‰4ä¸ªæ¨¡å‹
            
            if ensemble_result:
                # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
                comparison, success = validator.compare_with_paper_results(ensemble_result, 'model_ensemble')
                validation_results['comparisons']['ensemble'] = comparison
                validation_results['overall_success'] = success
        
        # ç”ŸæˆæŠ¥å‘Š
        report = validator.generate_reproducibility_report(validation_results)
        
        # ä¿å­˜æŠ¥å‘Š
        with open('reproducibility_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ å¯å¤ç°æ€§æŠ¥å‘Šå·²ä¿å­˜: reproducibility_report.json")
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ¯ éªŒè¯æ€»ç»“:")
        print(f"   ç¯å¢ƒéªŒè¯: âœ…")
        print(f"   æ¨¡å‹åŠ è½½: {'âœ…' if len(models) > 0 else 'âŒ'}")
        print(f"   ç»“æœå¯¹æ¯”: {'âœ…' if validation_results['overall_success'] else 'âŒ'}")
        print(f"   æ•´ä½“å¯å¤ç°æ€§: {'âœ… é€šè¿‡' if validation_results['overall_success'] else 'âŒ éœ€è¦è°ƒæ•´'}")
        
    else:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")

if __name__ == '__main__':
    import sys
    main()
