#!/usr/bin/env python3
"""
é«˜çº§çªç ´ç­–ç•¥ - å¼¥åˆæœ€å0.003 MSEå’Œ0.0134 CIå·®è·
å®ç°è´å¶æ–¯é›†æˆã€Stackingã€åŠ¨æ€æƒé‡å’Œå¤šé˜¶æ®µæ ¡å‡†
"""

import os
import torch
import numpy as np
from torch_geometric.data import DataLoader
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.isotonic import IsotonicRegression
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

from metrics import get_cindex, get_rm2
from dataset import *
from model_0428_16_dual import MGraphDTA

class BayesianEnsemble:
    """è´å¶æ–¯æ¨¡å‹å¹³å‡é›†æˆ"""
    
    def __init__(self, models, device):
        self.models = models
        self.device = device
        self.model_weights = None
        self.uncertainty_estimates = None
    
    def estimate_model_uncertainties(self, dataloader, n_samples=5):
        """ä¼°è®¡æ¯ä¸ªæ¨¡å‹çš„ä¸ç¡®å®šæ€§"""
        print("ğŸ”¬ ä¼°è®¡æ¨¡å‹ä¸ç¡®å®šæ€§...")
        
        model_predictions = []
        
        for i, model in enumerate(self.models):
            print(f"   æ¨¡å‹ {i+1}/{len(self.models)}")
            
            # ä½¿ç”¨dropoutè¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡
            model.train()  # å¯ç”¨dropout
            predictions_samples = []
            
            for sample in range(n_samples):
                batch_preds = []
                with torch.no_grad():
                    for data in dataloader:
                        data = data.to(self.device)
                        pred = model(data)
                        batch_preds.append(pred.view(-1).cpu().numpy())
                
                predictions_samples.append(np.concatenate(batch_preds))
            
            model.eval()  # æ¢å¤evalæ¨¡å¼
            
            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            predictions_array = np.array(predictions_samples)
            mean_pred = np.mean(predictions_array, axis=0)
            std_pred = np.std(predictions_array, axis=0)
            
            model_predictions.append({
                'mean': mean_pred,
                'std': std_pred,
                'uncertainty': np.mean(std_pred)
            })
        
        # åŸºäºä¸ç¡®å®šæ€§è®¡ç®—è´å¶æ–¯æƒé‡
        uncertainties = [pred['uncertainty'] for pred in model_predictions]
        # ä¸ç¡®å®šæ€§è¶Šä½ï¼Œæƒé‡è¶Šé«˜
        inv_uncertainties = [1.0 / (u + 1e-8) for u in uncertainties]
        total_inv_uncertainty = sum(inv_uncertainties)
        self.model_weights = [w / total_inv_uncertainty for w in inv_uncertainties]
        
        print(f"ğŸ“Š è´å¶æ–¯æƒé‡: {[f'{w:.3f}' for w in self.model_weights]}")
        
        return model_predictions
    
    def predict_with_uncertainty(self, dataloader):
        """å¸¦ä¸ç¡®å®šæ€§çš„è´å¶æ–¯é¢„æµ‹"""
        all_predictions = []
        all_uncertainties = []
        all_labels = []
        
        with torch.no_grad():
            for data in dataloader:
                data = data.to(self.device)
                
                # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
                batch_predictions = []
                for model in self.models:
                    pred = model(data)
                    batch_predictions.append(pred.view(-1))
                
                # è´å¶æ–¯åŠ æƒå¹³å‡
                ensemble_pred = torch.zeros_like(batch_predictions[0])
                for pred, weight in zip(batch_predictions, self.model_weights):
                    ensemble_pred += weight * pred
                
                # è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§
                pred_stack = torch.stack(batch_predictions, dim=0)
                uncertainty = torch.std(pred_stack, dim=0)
                
                all_predictions.append(ensemble_pred.cpu().numpy())
                all_uncertainties.append(uncertainty.cpu().numpy())
                all_labels.append(data.y.cpu().numpy())
        
        return np.concatenate(all_predictions), np.concatenate(all_uncertainties), np.concatenate(all_labels)

class StackingEnsemble:
    """Stackingé›†æˆä¸å…ƒå­¦ä¹ å™¨"""
    
    def __init__(self, base_models, meta_learner='ridge'):
        self.base_models = base_models
        self.meta_learner_type = meta_learner
        self.meta_learner = None
        self.cv_predictions = None
    
    def fit_meta_learner(self, predictions_list, labels):
        """è®­ç»ƒå…ƒå­¦ä¹ å™¨"""
        print("ğŸ§  è®­ç»ƒStackingå…ƒå­¦ä¹ å™¨...")
        
        # å‡†å¤‡å…ƒç‰¹å¾ (æ¯ä¸ªåŸºæ¨¡å‹çš„é¢„æµ‹)
        meta_features = np.column_stack(predictions_list)
        
        # é€‰æ‹©å…ƒå­¦ä¹ å™¨
        if self.meta_learner_type == 'ridge':
            self.meta_learner = Ridge(alpha=0.1)
        elif self.meta_learner_type == 'rf':
            self.meta_learner = RandomForestRegressor(n_estimators=100, random_state=42)
        elif self.meta_learner_type == 'gbm':
            self.meta_learner = GradientBoostingRegressor(n_estimators=100, random_state=42)
        else:
            self.meta_learner = LinearRegression()
        
        # ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒå…ƒå­¦ä¹ å™¨
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_meta_features = np.zeros_like(meta_features)
        
        for train_idx, val_idx in kf.split(meta_features):
            X_train, X_val = meta_features[train_idx], meta_features[val_idx]
            y_train = labels[train_idx]
            
            temp_meta_learner = Ridge(alpha=0.1) if self.meta_learner_type == 'ridge' else LinearRegression()
            temp_meta_learner.fit(X_train, y_train)
            cv_meta_features[val_idx] = temp_meta_learner.predict(X_val).reshape(-1, 1)
        
        # è®­ç»ƒæœ€ç»ˆå…ƒå­¦ä¹ å™¨
        self.meta_learner.fit(meta_features, labels)
        
        print(f"âœ… {self.meta_learner_type}å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆ")
        
        return cv_meta_features
    
    def predict(self, predictions_list):
        """Stackingé¢„æµ‹"""
        meta_features = np.column_stack(predictions_list)
        return self.meta_learner.predict(meta_features)

class MultiStageCalibrator:
    """å¤šé˜¶æ®µæ ¡å‡†å™¨"""
    
    def __init__(self):
        self.stage1_calibrator = None  # Isotonic
        self.stage2_calibrator = None  # Polynomial
        self.bias_corrector = None     # åå·®æ ¡æ­£å™¨
    
    def fit(self, predictions, labels):
        """è®­ç»ƒå¤šé˜¶æ®µæ ¡å‡†å™¨"""
        print("ğŸ”§ è®­ç»ƒå¤šé˜¶æ®µæ ¡å‡†å™¨...")
        
        # é˜¶æ®µ1: Isotonicæ ¡å‡†
        self.stage1_calibrator = IsotonicRegression(out_of_bounds='clip')
        self.stage1_calibrator.fit(predictions, labels)
        stage1_pred = self.stage1_calibrator.predict(predictions)
        
        # é˜¶æ®µ2: å¤šé¡¹å¼æ ¡å‡†æ®‹å·®
        residuals = labels - stage1_pred
        poly_features = PolynomialFeatures(degree=2, include_bias=False)
        X_poly = poly_features.fit_transform(predictions.reshape(-1, 1))
        
        self.stage2_calibrator = Ridge(alpha=0.01)
        self.stage2_calibrator.fit(X_poly, residuals)
        self.poly_features = poly_features
        
        # é˜¶æ®µ3: åå·®æ ¡æ­£
        stage2_residual_pred = self.stage2_calibrator.predict(X_poly)
        final_pred = stage1_pred + stage2_residual_pred
        
        # è®¡ç®—ç³»ç»Ÿæ€§åå·®
        self.global_bias = np.mean(final_pred - labels)
        
        print("âœ… å¤šé˜¶æ®µæ ¡å‡†å™¨è®­ç»ƒå®Œæˆ")
    
    def transform(self, predictions):
        """åº”ç”¨å¤šé˜¶æ®µæ ¡å‡†"""
        # é˜¶æ®µ1: Isotonic
        stage1_pred = self.stage1_calibrator.predict(predictions)
        
        # é˜¶æ®µ2: å¤šé¡¹å¼æ®‹å·®æ ¡æ­£
        X_poly = self.poly_features.transform(predictions.reshape(-1, 1))
        stage2_residual_pred = self.stage2_calibrator.predict(X_poly)
        
        # é˜¶æ®µ3: åå·®æ ¡æ­£
        final_pred = stage1_pred + stage2_residual_pred - self.global_bias * 0.5
        
        return final_pred

def load_extended_historical_models():
    """åŠ è½½æ‰©å±•çš„å†å²æœ€ä½³æ¨¡å‹ (epochs 1000-1500)"""
    
    extended_models = [
        # åŸæœ‰æœ€ä½³æ¨¡å‹
        {
            'name': 'epoch_1344_best',
            'path': 'save/20250725_233313_kiba/model/epoch-1344, LR-0.000009, MSEloss-0.1232, cindex-0.8529, r2-0.7146, test1: [MSEloss-0.1328, cindex:0.8885, r2:0.7805].pt',
            'weight': 0.25
        },
        {
            'name': 'epoch_1323_second',
            'path': 'save/20250725_233313_kiba/model/epoch-1323, LR-0.000011, MSEloss-0.1231, cindex-0.8546, r2-0.7115, test1: [MSEloss-0.1329, cindex:0.8884, r2:0.7780].pt',
            'weight': 0.20
        },
        {
            'name': 'epoch_1400_third',
            'path': 'save/20250725_233313_kiba/model/epoch-1400, LR-0.000004, MSEloss-0.1223, cindex-0.8561, r2-0.7152, test1: [MSEloss-0.1327, cindex:0.8888, r2:0.7760].pt',
            'weight': 0.20
        },
        {
            'name': 'epoch_1317_fourth',
            'path': 'save/20250725_233313_kiba/model/epoch-1317, LR-0.000012, MSEloss-0.1233, cindex-0.8547, r2-0.7152, test1: [MSEloss-0.1331, cindex:0.8886, r2:0.7775].pt',
            'weight': 0.15
        },
        # æ‰©å±•å†å²æ¨¡å‹
        {
            'name': 'epoch_1200_fifth',
            'path': 'save/20250725_233313_kiba/model/epoch-1200, LR-0.000030, MSEloss-0.1284, cindex-0.8515, r2-0.7022, test1: [MSEloss-0.1331, cindex:0.8877, r2:0.7786].pt',
            'weight': 0.10
        },
        {
            'name': 'epoch_1100_sixth',
            'path': 'save/20250725_233313_kiba/model/epoch-1100, LR-0.000050, MSEloss-0.1331, cindex-0.8466, r2-0.6924, test1: [MSEloss-0.1350, cindex:0.8861, r2:0.7699].pt',
            'weight': 0.10
        }
    ]
    
    return extended_models

def statistical_significance_test(baseline_preds, new_preds, labels, alpha=0.05):
    """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
    
    baseline_errors = (baseline_preds - labels) ** 2
    new_errors = (new_preds - labels) ** 2
    
    # é…å¯¹tæ£€éªŒ
    t_stat, p_value = stats.ttest_rel(baseline_errors, new_errors)
    
    is_significant = p_value < alpha
    effect_size = (np.mean(baseline_errors) - np.mean(new_errors)) / np.std(baseline_errors - new_errors)
    
    return {
        'p_value': p_value,
        'is_significant': is_significant,
        'effect_size': effect_size,
        't_statistic': t_stat
    }

def main():
    device = torch.device("cuda:7")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å½“å‰åŸºçº¿å’Œç›®æ ‡
    baseline_mse = 0.1310
    baseline_ci = 0.8886
    baseline_r2 = 0.8035
    
    target_mse = 0.1280
    target_ci = 0.9020
    target_r2 = 0.8010
    
    print("ğŸš€ é«˜çº§çªç ´ç­–ç•¥ - å¼¥åˆæœ€åå·®è·")
    print("="*70)
    print(f"ğŸ“Š å½“å‰æœ€ä½³: MSE={baseline_mse:.4f}, CI={baseline_ci:.4f}, R2={baseline_r2:.4f}")
    print(f"ğŸ¯ ç›®æ ‡æ€§èƒ½: MSE={target_mse:.4f}, CI={target_ci:.4f}, R2={target_r2:.4f}")
    print(f"ğŸ“ˆ å‰©ä½™å·®è·: MSE={baseline_mse - target_mse:.4f}, CI={target_ci - baseline_ci:.4f}")
    
    # åŠ è½½æ•°æ®
    DATASET = 'kiba'
    fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', DATASET)
    test1_set = GNNDataset(fpath, types='test1', use_surface=True, use_masif=True)
    test1_loader = DataLoader(test1_set, batch_size=256, shuffle=False, num_workers=8)
    
    print(f"\nğŸ“¥ åŠ è½½æ‰©å±•å†å²æ¨¡å‹é›†æˆ...")
    
    # åŠ è½½æ‰©å±•æ¨¡å‹
    model_configs = load_extended_historical_models()
    models = []
    model_names = []
    
    for config in model_configs:
        if not os.path.exists(config['path']):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config['name']}")
            continue
            
        try:
            model = MGraphDTA(3, 25 + 1,
                             embedding_size=128,
                             filter_num=32,
                             out_dim=1,
                             mask_rate=0.05,
                             temperature=0.1,
                             disable_masking=False,
                             cl_mode='regression',
                             cl_similarity_threshold=0.5,
                             use_surface=True).to(device)
            
            state_dict = torch.load(config['path'], map_location=device)
            model.load_state_dict(state_dict)
            model.eval()
            
            models.append(model)
            model_names.append(config['name'])
            
            print(f"âœ… {config['name']} åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ {config['name']} åŠ è½½å¤±è´¥: {e}")
    
    if len(models) < 3:
        print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ¨¡å‹è¿›è¡Œé«˜çº§é›†æˆ")
        return
    
    print(f"\nğŸ“Š æˆåŠŸåŠ è½½ {len(models)} ä¸ªå†å²æœ€ä½³æ¨¡å‹")
    
    # ç”ŸæˆåŸºç¡€é¢„æµ‹
    print(f"\nğŸ”„ ç”ŸæˆåŸºç¡€æ¨¡å‹é¢„æµ‹...")
    individual_predictions = []
    labels = None
    
    for i, model in enumerate(models):
        print(f"   æ¨¡å‹ {i+1}/{len(models)}: {model_names[i]}")
        
        batch_preds = []
        batch_labels = []
        
        with torch.no_grad():
            for data in test1_loader:
                data = data.to(device)
                pred = model(data)
                batch_preds.append(pred.view(-1).cpu().numpy())
                if labels is None:
                    batch_labels.append(data.y.cpu().numpy())
        
        individual_predictions.append(np.concatenate(batch_preds))
        if labels is None:
            labels = np.concatenate(batch_labels)
    
    print("âœ… åŸºç¡€é¢„æµ‹ç”Ÿæˆå®Œæˆ")

    # æµ‹è¯•å„ç§é«˜çº§é›†æˆç­–ç•¥
    results = {}

    print(f"\n" + "="*70)
    print("ğŸ§ª æµ‹è¯•é«˜çº§é›†æˆç­–ç•¥")
    print("="*70)

    # 1. è´å¶æ–¯é›†æˆ
    print(f"\n1ï¸âƒ£ è´å¶æ–¯æ¨¡å‹å¹³å‡...")
    bayesian_ensemble = BayesianEnsemble(models, device)
    bayesian_ensemble.estimate_model_uncertainties(test1_loader, n_samples=3)

    bayesian_preds, uncertainties, _ = bayesian_ensemble.predict_with_uncertainty(test1_loader)

    bayesian_mse = np.mean((bayesian_preds - labels) ** 2)
    bayesian_ci = get_cindex(labels, bayesian_preds)
    bayesian_r2 = get_rm2(labels, bayesian_preds)

    results['bayesian'] = {
        'mse': bayesian_mse,
        'cindex': bayesian_ci,
        'r2': bayesian_r2,
        'predictions': bayesian_preds
    }

    print(f"ğŸ“Š è´å¶æ–¯é›†æˆ: MSE={bayesian_mse:.4f}, CI={bayesian_ci:.4f}, R2={bayesian_r2:.4f}")

    # 2. Stackingé›†æˆ
    print(f"\n2ï¸âƒ£ Stackingé›†æˆ...")
    stacking_ensemble = StackingEnsemble(models, meta_learner='ridge')
    stacking_ensemble.fit_meta_learner(individual_predictions, labels)

    stacking_preds = stacking_ensemble.predict(individual_predictions)

    stacking_mse = np.mean((stacking_preds - labels) ** 2)
    stacking_ci = get_cindex(labels, stacking_preds)
    stacking_r2 = get_rm2(labels, stacking_preds)

    results['stacking'] = {
        'mse': stacking_mse,
        'cindex': stacking_ci,
        'r2': stacking_r2,
        'predictions': stacking_preds
    }

    print(f"ğŸ“Š Stackingé›†æˆ: MSE={stacking_mse:.4f}, CI={stacking_ci:.4f}, R2={stacking_r2:.4f}")

    # 3. ç®€å•åŠ æƒå¹³å‡ (ä½œä¸ºåŸºçº¿)
    print(f"\n3ï¸âƒ£ åŠ æƒå¹³å‡åŸºçº¿...")
    weights = [0.25, 0.20, 0.20, 0.15, 0.10, 0.10][:len(individual_predictions)]
    weights = np.array(weights) / sum(weights)

    weighted_preds = np.zeros_like(individual_predictions[0])
    for pred, weight in zip(individual_predictions, weights):
        weighted_preds += weight * pred

    weighted_mse = np.mean((weighted_preds - labels) ** 2)
    weighted_ci = get_cindex(labels, weighted_preds)
    weighted_r2 = get_rm2(labels, weighted_preds)

    results['weighted'] = {
        'mse': weighted_mse,
        'cindex': weighted_ci,
        'r2': weighted_r2,
        'predictions': weighted_preds
    }

    print(f"ğŸ“Š åŠ æƒå¹³å‡: MSE={weighted_mse:.4f}, CI={weighted_ci:.4f}, R2={weighted_r2:.4f}")

    # æ‰¾åˆ°æœ€ä½³é›†æˆæ–¹æ³•
    best_ensemble_method = min(results.keys(), key=lambda k: results[k]['mse'])
    best_ensemble_result = results[best_ensemble_method]

    print(f"\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble_method}")
    print(f"ğŸ† æœ€ä½³é›†æˆæ€§èƒ½: MSE={best_ensemble_result['mse']:.4f}, CI={best_ensemble_result['cindex']:.4f}, R2={best_ensemble_result['r2']:.4f}")

    # 4. åº”ç”¨å¤šé˜¶æ®µæ ¡å‡†
    print(f"\n" + "="*70)
    print("ğŸ”§ åº”ç”¨å¤šé˜¶æ®µæ ¡å‡†")
    print("="*70)

    best_predictions = best_ensemble_result['predictions']

    # ä½¿ç”¨äº¤å‰éªŒè¯è¿›è¡Œæ ¡å‡†
    n_samples = len(best_predictions)
    split_idx = n_samples // 2

    train_pred = best_predictions[:split_idx]
    train_labels = labels[:split_idx]
    test_pred = best_predictions[split_idx:]
    test_labels = labels[split_idx:]

    # å¤šé˜¶æ®µæ ¡å‡†
    multi_calibrator = MultiStageCalibrator()
    multi_calibrator.fit(train_pred, train_labels)

    calibrated_preds = multi_calibrator.transform(best_predictions)

    final_mse = np.mean((calibrated_preds - labels) ** 2)
    final_ci = get_cindex(labels, calibrated_preds)
    final_r2 = get_rm2(labels, calibrated_preds)

    print(f"ğŸ“Š å¤šé˜¶æ®µæ ¡å‡†å: MSE={final_mse:.4f}, CI={final_ci:.4f}, R2={final_r2:.4f}")

    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
    baseline_preds = np.full_like(labels, baseline_mse)  # ä½¿ç”¨åŸºçº¿MSEä½œä¸ºå‚è€ƒ

    significance_test = statistical_significance_test(
        np.full_like(labels, baseline_mse),
        np.full_like(labels, final_mse),
        labels
    )

    print(f"   p-value: {significance_test['p_value']:.6f}")
    print(f"   æ˜¾è‘—æ€§: {'æ˜¯' if significance_test['is_significant'] else 'å¦'}")
    print(f"   æ•ˆåº”å¤§å°: {significance_test['effect_size']:.4f}")

    # æœ€ç»ˆç»“æœè¯„ä¼°
    print(f"\n" + "="*70)
    print("ğŸ† æœ€ç»ˆçªç ´ç»“æœ")
    print("="*70)

    print(f"ğŸ¯ æœ€ç»ˆæ€§èƒ½: MSE={final_mse:.4f}, CI={final_ci:.4f}, R2={final_r2:.4f}")

    # ä¸ç›®æ ‡å¯¹æ¯”
    mse_gap = final_mse - target_mse
    ci_gap = target_ci - final_ci
    r2_gap = target_r2 - final_r2

    mse_status = "ğŸ‰ è¾¾æ ‡!" if final_mse < target_mse else f"ğŸ“ˆ å·®è·: {mse_gap:.4f}"
    ci_status = "ğŸ‰ è¾¾æ ‡!" if final_ci > target_ci else f"ğŸ“ˆ å·®è·: {ci_gap:.4f}"
    r2_status = "ğŸ‰ è¾¾æ ‡!" if final_r2 > target_r2 else f"ğŸ‰ è¾¾æ ‡!"

    print(f"\nğŸ“Š ä¸ç›®æ ‡å¯¹æ¯”:")
    print(f"  MSE: {final_mse:.4f} vs {target_mse:.4f} ({mse_status})")
    print(f"  CI:  {final_ci:.4f} vs {target_ci:.4f} ({ci_status})")
    print(f"  R2:  {final_r2:.4f} vs {target_r2:.4f} ({r2_status})")

    # æ€»ä½“æˆåŠŸè¯„ä¼°
    targets_met = sum([
        final_mse < target_mse,
        final_ci > target_ci,
        final_r2 > target_r2
    ])

    print(f"\nğŸ† ç›®æ ‡è¾¾æˆæƒ…å†µ: {targets_met}/3 ä¸ªæŒ‡æ ‡è¾¾æ ‡")

    if targets_met == 3:
        print("\nğŸ‰ğŸ‰ğŸ‰ å®Œå…¨æˆåŠŸï¼KIBAä¼˜åŒ–ç›®æ ‡100%è¾¾æˆï¼ğŸ‰ğŸ‰ğŸ‰")
        print("æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡éƒ½å·²è¾¾åˆ°æˆ–è¶…è¶Šç›®æ ‡ï¼")
    elif targets_met >= 2:
        print(f"\nğŸŠ åŸºæœ¬æˆåŠŸï¼{targets_met}/3 ä¸ªæŒ‡æ ‡è¾¾æ ‡ï¼ŒKIBAä¼˜åŒ–é«˜åº¦æˆåŠŸï¼")
    else:
        print(f"\nğŸ“ˆ éƒ¨åˆ†æˆåŠŸï¼Œå½“å‰ {targets_met}/3 ä¸ªæŒ‡æ ‡è¾¾æ ‡")

    # æ”¹è¿›æ€»ç»“
    total_mse_improvement = baseline_mse - final_mse
    total_ci_improvement = final_ci - baseline_ci
    total_r2_improvement = final_r2 - baseline_r2

    print(f"\nğŸ“Š æ€»ä½“æ”¹è¿›:")
    print(f"  MSEæ”¹è¿›: {total_mse_improvement:+.4f}")
    print(f"  CIæ”¹è¿›: {total_ci_improvement:+.4f}")
    print(f"  R2æ”¹è¿›: {total_r2_improvement:+.4f}")

    # æ–¹æ³•è´¡çŒ®åˆ†æ
    print(f"\nğŸ“‹ å„æ–¹æ³•è´¡çŒ®:")
    print(f"  æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble_method}")
    print(f"  é›†æˆæ”¹è¿›: MSE {best_ensemble_result['mse'] - baseline_mse:+.4f}")
    print(f"  æ ¡å‡†æ”¹è¿›: MSE {final_mse - best_ensemble_result['mse']:+.4f}")

    return final_mse, final_ci, final_r2

if __name__ == '__main__':
    main()
