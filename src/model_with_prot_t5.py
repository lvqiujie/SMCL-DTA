#!/usr/bin/env python3
"""
ProtT5å¢å¼ºçš„MGraphDTAæ¨¡å‹
åœ¨åŸæœ‰æ¶æ„åŸºç¡€ä¸Šé›†æˆProtT5è›‹ç™½è´¨åµŒå…¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool

# å¯¼å…¥åŸå§‹æ¨¡å‹
from model_0428_16_dual import MGraphDTA

class ProtT5FusionModule(nn.Module):
    """ProtT5ç‰¹å¾èåˆæ¨¡å—"""
    
    def __init__(self, prot_t5_dim=1024, protein_dim=96, fusion_dim=128):
        super().__init__()
        
        self.prot_t5_dim = prot_t5_dim
        self.protein_dim = protein_dim
        self.fusion_dim = fusion_dim
        
        # ProtT5åµŒå…¥å¤„ç†
        self.prot_t5_projector = nn.Sequential(
            nn.Linear(prot_t5_dim, fusion_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.LayerNorm(fusion_dim)
        )
        
        # åŸå§‹è›‹ç™½è´¨ç‰¹å¾å¤„ç†
        self.protein_projector = nn.Sequential(
            nn.Linear(protein_dim, fusion_dim),
            nn.ReLU(),
            nn.LayerNorm(fusion_dim)
        )
        
        # å¤šæ¨¡æ€èåˆ
        self.fusion_attention = nn.MultiheadAttention(
            embed_dim=fusion_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # èåˆåçš„ç‰¹å¾å¤„ç†
        self.fusion_output = nn.Sequential(
            nn.Linear(fusion_dim, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_dim, protein_dim)  # è¾“å‡ºç»´åº¦ä¸åŸå§‹è›‹ç™½è´¨ç‰¹å¾ä¸€è‡´
        )
        
    def forward(self, protein_features, prot_t5_embeddings):
        """
        Args:
            protein_features: [batch_size, protein_dim] åŸå§‹è›‹ç™½è´¨ç‰¹å¾
            prot_t5_embeddings: [batch_size, prot_t5_dim] ProtT5åµŒå…¥
        Returns:
            fused_features: [batch_size, protein_dim] èåˆåçš„è›‹ç™½è´¨ç‰¹å¾
        """
        batch_size = protein_features.size(0)
        
        # å¤„ç†ProtT5åµŒå…¥
        prot_t5_proj = self.prot_t5_projector(prot_t5_embeddings)  # [batch_size, fusion_dim]
        
        # å¤„ç†åŸå§‹è›‹ç™½è´¨ç‰¹å¾
        protein_proj = self.protein_projector(protein_features)    # [batch_size, fusion_dim]
        
        # å‡†å¤‡å¤šå¤´æ³¨æ„åŠ›è¾“å…¥ [batch_size, seq_len, embed_dim]
        # è¿™é‡Œseq_len=2ï¼Œåˆ†åˆ«å¯¹åº”ProtT5å’ŒåŸå§‹ç‰¹å¾
        multi_modal_input = torch.stack([prot_t5_proj, protein_proj], dim=1)  # [batch_size, 2, fusion_dim]
        
        # å¤šå¤´æ³¨æ„åŠ›èåˆ
        fused_output, attention_weights = self.fusion_attention(
            multi_modal_input, multi_modal_input, multi_modal_input
        )  # [batch_size, 2, fusion_dim]
        
        # èšåˆèåˆç»“æœ
        aggregated = fused_output.mean(dim=1)  # [batch_size, fusion_dim]
        
        # è¾“å‡ºå¤„ç†
        final_features = self.fusion_output(aggregated)  # [batch_size, protein_dim]
        
        return final_features

class MGraphDTAWithProtT5(MGraphDTA):
    """é›†æˆProtT5çš„å¢å¼ºMGraphDTAæ¨¡å‹"""
    
    def __init__(self, num_features_mol, num_features_pro, embedding_size=128, 
                 filter_num=32, out_dim=1, mask_rate=0.05, temperature=0.1,
                 disable_masking=False, cl_mode='regression', cl_similarity_threshold=0.5,
                 use_surface=True, use_prot_t5=True, prot_t5_fusion_dim=128):
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            num_features_mol, num_features_pro, embedding_size, filter_num, out_dim,
            mask_rate, temperature, disable_masking, cl_mode, cl_similarity_threshold, use_surface
        )
        
        self.use_prot_t5 = use_prot_t5
        
        if use_prot_t5:
            # æ·»åŠ ProtT5èåˆæ¨¡å—
            self.prot_t5_fusion = ProtT5FusionModule(
                prot_t5_dim=1024,
                protein_dim=num_features_pro,
                fusion_dim=prot_t5_fusion_dim
            )
            
            print(f"âœ… ProtT5èåˆæ¨¡å—å·²æ·»åŠ ")
            print(f"   - ProtT5ç»´åº¦: 1024")
            print(f"   - è›‹ç™½è´¨ç‰¹å¾ç»´åº¦: {num_features_pro}")
            print(f"   - èåˆç»´åº¦: {prot_t5_fusion_dim}")
    
    def forward(self, data):
        """å‰å‘ä¼ æ’­ - é›†æˆProtT5ç‰¹å¾"""
        
        # åˆ†å­å›¾ç¼–ç  (ä¿æŒåŸæœ‰é€»è¾‘)
        mol_x, mol_edge_index, mol_batch = data.x, data.edge_index, data.batch
        
        # åˆ†å­ç‰¹å¾æå–
        mol_x = self.mol_conv1(mol_x, mol_edge_index)
        mol_x = F.relu(mol_x)
        mol_x = self.mol_conv2(mol_x, mol_edge_index)
        mol_x = F.relu(mol_x)
        mol_x = self.mol_conv3(mol_x, mol_edge_index)
        
        # åˆ†å­å›¾æ± åŒ–
        mol_x = torch.cat([global_mean_pool(mol_x, mol_batch), 
                          global_max_pool(mol_x, mol_batch)], dim=1)
        
        # è›‹ç™½è´¨ç‰¹å¾å¤„ç†
        if hasattr(data, 'target') and data.target is not None:
            pro_x = data.target
        else:
            # å¦‚æœæ²¡æœ‰targetå­—æ®µï¼Œä½¿ç”¨é»˜è®¤å¤„ç†
            batch_size = mol_x.size(0)
            pro_x = torch.randn(batch_size, self.num_features_pro, device=mol_x.device)
        
        # ProtT5ç‰¹å¾èåˆ
        if self.use_prot_t5 and hasattr(data, 'prot_t5_embedding'):
            prot_t5_emb = data.prot_t5_embedding
            
            # ç¡®ä¿æ‰¹æ¬¡ç»´åº¦åŒ¹é…
            if prot_t5_emb.size(0) != pro_x.size(0):
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œè°ƒæ•´
                if prot_t5_emb.dim() == 1:
                    prot_t5_emb = prot_t5_emb.unsqueeze(0).repeat(pro_x.size(0), 1)
                elif prot_t5_emb.size(0) == 1 and pro_x.size(0) > 1:
                    prot_t5_emb = prot_t5_emb.repeat(pro_x.size(0), 1)
            
            # åº”ç”¨ProtT5èåˆ
            pro_x = self.prot_t5_fusion(pro_x, prot_t5_emb)
        
        # è›‹ç™½è´¨ç‰¹å¾ç¼–ç 
        pro_x = F.relu(self.pro_fc1(pro_x))
        pro_x = F.dropout(pro_x, training=self.training)
        pro_x = F.relu(self.pro_fc2(pro_x))
        pro_x = F.dropout(pro_x, training=self.training)
        pro_x = F.relu(self.pro_fc3(pro_x))
        
        # åˆ†å­-è›‹ç™½è´¨ç‰¹å¾èåˆ
        combined = torch.cat([mol_x, pro_x], dim=1)
        
        # æœ€ç»ˆé¢„æµ‹
        combined = F.relu(self.fc1(combined))
        combined = F.dropout(combined, training=self.training)
        combined = F.relu(self.fc2(combined))
        combined = F.dropout(combined, training=self.training)
        output = self.out(combined)
        
        return output
    
    def get_feature_dimensions(self):
        """è·å–ç‰¹å¾ç»´åº¦ä¿¡æ¯"""
        info = {
            'molecular_features': self.num_features_mol,
            'protein_features': self.num_features_pro,
            'embedding_size': self.embedding_size,
            'use_prot_t5': self.use_prot_t5
        }
        
        if self.use_prot_t5:
            info['prot_t5_dim'] = 1024
            info['fusion_dim'] = self.prot_t5_fusion.fusion_dim
        
        return info

def create_enhanced_model(num_features_mol=3, num_features_pro=97, use_prot_t5=True, **kwargs):
    """åˆ›å»ºProtT5å¢å¼ºçš„æ¨¡å‹"""
    
    model = MGraphDTAWithProtT5(
        num_features_mol=num_features_mol,
        num_features_pro=num_features_pro,
        use_prot_t5=use_prot_t5,
        **kwargs
    )
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ—ï¸ ProtT5å¢å¼ºæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   - æ€»å‚æ•°: {total_params:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   - ProtT5é›†æˆ: {use_prot_t5}")
    
    return model

if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    print("ğŸ§ª æµ‹è¯•ProtT5å¢å¼ºæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_enhanced_model(
        num_features_mol=3,
        num_features_pro=97,  # 96 + 1 (åŸå§‹ç‰¹å¾)
        embedding_size=128,
        filter_num=32,
        use_prot_t5=True
    )
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    feature_info = model.get_feature_dimensions()
    print(f"\nğŸ“Š ç‰¹å¾ç»´åº¦ä¿¡æ¯:")
    for key, value in feature_info.items():
        print(f"   - {key}: {value}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    num_nodes = 20
    
    # æ¨¡æ‹Ÿå›¾æ•°æ®
    x = torch.randn(num_nodes * batch_size, 3)
    edge_index = torch.randint(0, num_nodes * batch_size, (2, num_nodes * batch_size * 2))
    batch = torch.repeat_interleave(torch.arange(batch_size), num_nodes)
    target = torch.randn(batch_size, 97)
    prot_t5_embedding = torch.randn(batch_size, 1024)
    y = torch.randn(batch_size, 1)
    
    # åˆ›å»ºæ•°æ®å¯¹è±¡
    from torch_geometric.data import Data
    test_data = Data(
        x=x,
        edge_index=edge_index,
        batch=batch,
        target=target,
        prot_t5_embedding=prot_t5_embedding,
        y=y
    )
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(test_data)
        print(f"\nğŸ”„ å‰å‘ä¼ æ’­æµ‹è¯•:")
        print(f"   - è¾“å…¥å½¢çŠ¶: åˆ†å­èŠ‚ç‚¹={x.shape}, è›‹ç™½è´¨={target.shape}, ProtT5={prot_t5_embedding.shape}")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   - è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
    
    print("âœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")
