#!/usr/bin/env python3
"""
å¢å¼ºçš„æ•°æ®é›†åŠ è½½å™¨ - é›†æˆProtT5è›‹ç™½è´¨åµŒå…¥
åœ¨åŸæœ‰ç‰¹å¾åŸºç¡€ä¸Šæ·»åŠ ProtT5é¢„è®­ç»ƒè›‹ç™½è´¨è¡¨ç¤º
"""

import os
import torch
import numpy as np
import pandas as pd
import pickle
from torch_geometric.data import Data, Dataset
from transformers import T5EncoderModel, T5Tokenizer
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥åŸå§‹æ•°æ®é›†ç±»
from dataset import GNNDataset

class ProtT5EmbeddingGenerator:
    """ProtT5åµŒå…¥ç”Ÿæˆå™¨"""
    
    def __init__(self, model_path="/home/lww/prot_t5_model", device='cpu'):
        self.device = torch.device(device)
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.embedding_cache = {}
        
    def load_model(self):
        """åŠ è½½ProtT5æ¨¡å‹"""
        try:
            print(f"ğŸ”„ åŠ è½½ProtT5æ¨¡å‹ä»: {self.model_path}")
            self.model = T5EncoderModel.from_pretrained(self.model_path, local_files_only=True)
            self.tokenizer = T5Tokenizer.from_pretrained(self.model_path, do_lower_case=False, local_files_only=True)
            self.model = self.model.to(self.device)
            self.model.eval()
            print("âœ… ProtT5æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ ProtT5æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å°†ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥æˆ–éšæœºåˆå§‹åŒ–")
            return False
    
    def get_embedding(self, sequence):
        """è·å–å•ä¸ªè›‹ç™½è´¨åºåˆ—çš„ProtT5åµŒå…¥"""
        if sequence in self.embedding_cache:
            return self.embedding_cache[sequence]
        
        if self.model is None:
            # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›éšæœºåµŒå…¥ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            embedding = np.random.normal(0, 0.1, 1024).astype(np.float32)
            self.embedding_cache[sequence] = embedding
            return embedding
        
        try:
            # å¤„ç†ç‰¹æ®Šæ°¨åŸºé…¸
            sequence = sequence.replace('U', 'X').replace('Z', 'X').replace('O', 'X')
            
            # ProtT5éœ€è¦æ°¨åŸºé…¸ä¹‹é—´åŠ ç©ºæ ¼
            spaced_sequence = ' '.join(list(sequence))
            
            # ç¼–ç åºåˆ—
            inputs = self.tokenizer(spaced_sequence, return_tensors="pt", padding=True, truncation=True, max_length=1024)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # è·å–åµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨[CLS] tokençš„åµŒå…¥æˆ–å¹³å‡æ± åŒ–
                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
            
            # ç¼“å­˜ç»“æœ
            self.embedding_cache[sequence] = embedding.astype(np.float32)
            return embedding
            
        except Exception as e:
            print(f"âš ï¸ åºåˆ—åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            embedding = np.zeros(1024, dtype=np.float32)
            self.embedding_cache[sequence] = embedding
            return embedding

class GNNDatasetWithProtT5(GNNDataset):
    """å¢å¼ºçš„GNNæ•°æ®é›† - é›†æˆProtT5åµŒå…¥"""
    
    def __init__(self, root, types, use_surface=True, use_masif=True, use_prot_t5=True, 
                 prot_t5_model_path="/home/lww/prot_t5_model", device='cpu'):
        
        self.use_prot_t5 = use_prot_t5
        self.prot_t5_generator = None
        self.protein_embeddings = {}
        
        if use_prot_t5:
            self.prot_t5_generator = ProtT5EmbeddingGenerator(prot_t5_model_path, device)
            # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥
            model_loaded = self.prot_t5_generator.load_model()
            
            if not model_loaded:
                # å°è¯•åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥
                self._load_precomputed_embeddings(root)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(root, types, use_surface, use_masif)
        
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è¡¨é¢ç‰¹å¾: {use_surface}")
        print(f"   - MaSIFç‰¹å¾: {use_masif}")
        print(f"   - ProtT5åµŒå…¥: {use_prot_t5}")
    
    def _load_precomputed_embeddings(self, root):
        """åŠ è½½é¢„è®¡ç®—çš„ProtT5åµŒå…¥"""
        embedding_paths = [
            os.path.join(root, "protein_embeddings.npy"),
            os.path.join(root, "saved_protein_data", "protein_embeddings.npy"),
            "saved_protein_data/protein_embeddings.npy",
            "protein_embeddings.npy"
        ]
        
        protein_list_paths = [
            os.path.join(root, "protein_list.pkl"),
            os.path.join(root, "saved_protein_data", "protein_list.pkl"),
            "saved_protein_data/protein_list.pkl",
            "protein_list.pkl"
        ]
        
        for emb_path, prot_path in zip(embedding_paths, protein_list_paths):
            try:
                if os.path.exists(emb_path) and os.path.exists(prot_path):
                    embeddings = np.load(emb_path)
                    with open(prot_path, 'rb') as f:
                        protein_list = pickle.load(f)
                    
                    # åˆ›å»ºè›‹ç™½è´¨åºåˆ—åˆ°åµŒå…¥çš„æ˜ å°„
                    for protein, embedding in zip(protein_list, embeddings):
                        self.protein_embeddings[protein] = embedding.astype(np.float32)
                    
                    print(f"âœ… åŠ è½½é¢„è®¡ç®—ProtT5åµŒå…¥: {len(self.protein_embeddings)} ä¸ªè›‹ç™½è´¨")
                    return True
                    
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é¢„è®¡ç®—åµŒå…¥å¤±è´¥ ({emb_path}): {e}")
                continue
        
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®¡ç®—çš„ProtT5åµŒå…¥ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        return False
    
    def get_protein_embedding(self, protein_sequence):
        """è·å–è›‹ç™½è´¨çš„ProtT5åµŒå…¥"""
        if not self.use_prot_t5:
            return None
        
        # é¦–å…ˆæ£€æŸ¥é¢„è®¡ç®—çš„åµŒå…¥
        if protein_sequence in self.protein_embeddings:
            return self.protein_embeddings[protein_sequence]
        
        # å¦‚æœæœ‰ProtT5ç”Ÿæˆå™¨ï¼Œä½¿ç”¨å®ƒç”ŸæˆåµŒå…¥
        if self.prot_t5_generator:
            embedding = self.prot_t5_generator.get_embedding(protein_sequence)
            self.protein_embeddings[protein_sequence] = embedding
            return embedding
        
        # æœ€åçš„fallbackï¼šéšæœºåµŒå…¥
        embedding = np.random.normal(0, 0.1, 1024).astype(np.float32)
        self.protein_embeddings[protein_sequence] = embedding
        return embedding
    
    def get(self, idx):
        """é‡å†™getæ–¹æ³•ä»¥åŒ…å«ProtT5åµŒå…¥"""
        # è·å–åŸå§‹æ•°æ®
        data = super().get(idx)
        
        # æ·»åŠ ProtT5åµŒå…¥
        if self.use_prot_t5:
            # ä»æ•°æ®ä¸­è·å–è›‹ç™½è´¨åºåˆ—ï¼ˆéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´ï¼‰
            # è¿™é‡Œå‡è®¾è›‹ç™½è´¨åºåˆ—å­˜å‚¨åœ¨æŸä¸ªåœ°æ–¹ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
            try:
                # å°è¯•ä»æ–‡ä»¶åæˆ–å…¶ä»–æ–¹å¼è·å–è›‹ç™½è´¨åºåˆ—
                # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°ï¼Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
                protein_sequence = self._get_protein_sequence_for_idx(idx)
                
                if protein_sequence:
                    prot_t5_embedding = self.get_protein_embedding(protein_sequence)
                    data.prot_t5_embedding = torch.tensor(prot_t5_embedding, dtype=torch.float32)
                else:
                    # å¦‚æœæ— æ³•è·å–åºåˆ—ï¼Œä½¿ç”¨é›¶å‘é‡
                    data.prot_t5_embedding = torch.zeros(1024, dtype=torch.float32)
                    
            except Exception as e:
                print(f"âš ï¸ è·å–ProtT5åµŒå…¥å¤±è´¥ (idx={idx}): {e}")
                data.prot_t5_embedding = torch.zeros(1024, dtype=torch.float32)
        
        return data
    
    def _get_protein_sequence_for_idx(self, idx):
        """æ ¹æ®ç´¢å¼•è·å–è›‹ç™½è´¨åºåˆ— - éœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„å®ç°"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦æ–¹æ³•ï¼Œéœ€è¦æ ¹æ®å®é™…çš„æ•°æ®å­˜å‚¨æ–¹å¼æ¥å®ç°
        # å¯èƒ½éœ€è¦è¯»å–CSVæ–‡ä»¶æˆ–å…¶ä»–æ•°æ®æºæ¥è·å–è›‹ç™½è´¨åºåˆ—
        
        try:
            # å°è¯•ä»æ•°æ®æ–‡ä»¶ä¸­è¯»å–è›‹ç™½è´¨åºåˆ—
            data_file = os.path.join(self.root, 'raw', 'data.csv')
            if os.path.exists(data_file):
                df = pd.read_csv(data_file)
                # æ ¹æ®å®é™…çš„æ•°æ®ç»“æ„è°ƒæ•´è¿™é‡Œçš„é€»è¾‘
                # è¿™é‡Œå‡è®¾æœ‰ä¸€ä¸ªprotein_sequenceåˆ—
                if idx < len(df) and 'protein_sequence' in df.columns:
                    return df.iloc[idx]['protein_sequence']
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–è›‹ç™½è´¨åºåˆ—å¤±è´¥: {e}")
            return None

def create_enhanced_dataloaders(dataset='kiba', batch_size=256, use_prot_t5=True, 
                               prot_t5_model_path="/home/lww/prot_t5_model", device='cpu'):
    """åˆ›å»ºå¢å¼ºçš„æ•°æ®åŠ è½½å™¨"""
    
    fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', dataset)
    
    print(f"ğŸ”„ åˆ›å»ºå¢å¼ºæ•°æ®åŠ è½½å™¨...")
    print(f"   - æ•°æ®é›†: {dataset}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - ProtT5åµŒå…¥: {use_prot_t5}")
    
    # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    train_set = GNNDatasetWithProtT5(
        fpath, types='train', 
        use_surface=True, use_masif=True, use_prot_t5=use_prot_t5,
        prot_t5_model_path=prot_t5_model_path, device=device
    )
    
    test_set = GNNDatasetWithProtT5(
        fpath, types='test1',
        use_surface=True, use_masif=True, use_prot_t5=use_prot_t5,
        prot_t5_model_path=prot_t5_model_path, device=device
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch_geometric.data import DataLoader
    
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   - è®­ç»ƒé›†: {len(train_set)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
    
    return train_loader, test_loader

if __name__ == '__main__':
    # æµ‹è¯•å¢å¼ºæ•°æ®åŠ è½½å™¨
    train_loader, test_loader = create_enhanced_dataloaders(
        dataset='kiba', 
        batch_size=32,  # å°æ‰¹æ¬¡ç”¨äºæµ‹è¯•
        use_prot_t5=True,
        device='cpu'
    )
    
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
    for i, data in enumerate(train_loader):
        print(f"æ‰¹æ¬¡ {i+1}:")
        print(f"  - åˆ†å­ç‰¹å¾: {data.x.shape}")
        print(f"  - è¾¹ç´¢å¼•: {data.edge_index.shape}")
        if hasattr(data, 'prot_t5_embedding'):
            print(f"  - ProtT5åµŒå…¥: {data.prot_t5_embedding.shape}")
        print(f"  - æ ‡ç­¾: {data.y.shape}")
        
        if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
            break
    
    print("âœ… æ•°æ®åŠ è½½æµ‹è¯•å®Œæˆ")
