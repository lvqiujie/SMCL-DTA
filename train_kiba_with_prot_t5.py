#!/usr/bin/env python3
"""
KIBA ProtT5å¢å¼ºè®­ç»ƒè„šæœ¬
é›†æˆProtT5è›‹ç™½è´¨åµŒå…¥çš„å•æ¨¡å‹è®­ç»ƒæ–¹æ³•
ç›®æ ‡: çªç ´MSE<0.128çš„æœ€ç»ˆæ€§èƒ½ç“¶é¢ˆ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import argparse
import logging
from datetime import datetime

from src.model_with_prot_t5 import create_enhanced_model
from mydta.src.dataset_with_prot_t5 import create_enhanced_dataloaders
from src.metrics import get_cindex, get_rm2

def set_reproducible_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å®Œå…¨å¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

class ProtT5EnhancedTrainer:
    """ProtT5å¢å¼ºè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # ProtT5å¢å¼ºé…ç½®
        self.enhanced_config = {
            # åŸºç¡€é…ç½® (ç»§æ‰¿è‡ªå•æ¨¡å‹æ–¹æ³•)
            'embedding_size': 128,
            'filter_num': 32,
            'mask_rate': 0.05,
            'temperature': 0.1,
            'cl_similarity_threshold': 0.5,
            'contrastive_weight': 0.03,
            
            # ProtT5ç‰¹å®šé…ç½®
            'use_prot_t5': args.use_prot_t5,
            'prot_t5_fusion_dim': 128,
            'prot_t5_model_path': args.prot_t5_model_path,
            
            # è®­ç»ƒé…ç½® (é’ˆå¯¹ProtT5è°ƒæ•´)
            'lr': 2e-4,                  # ç•¥å¾®é™ä½å­¦ä¹ ç‡ (ProtT5ç‰¹å¾æ›´ä¸°å¯Œ)
            'weight_decay': 1e-4,
            'batch_size': 128,           # å‡å°æ‰¹æ¬¡å¤§å° (ProtT5å¢åŠ å†…å­˜ä½¿ç”¨)
            'max_epochs': 1200,          # å¯èƒ½éœ€è¦æ›´å°‘çš„epoch
            'early_stop_patience': 100,
            'grad_clip_norm': 1.0,
            
            # æ€§èƒ½ç›®æ ‡
            'target_mse': 0.128,         # æœ€ç»ˆç›®æ ‡
            'current_best_mse': 0.1310   # å½“å‰æœ€ä½³æ€§èƒ½
        }
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–ProtT5å¢å¼ºè®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“Š å¢å¼ºé…ç½®: {self.enhanced_config}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = f"logs/prot_t5_enhanced_{timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{log_dir}/training.log"),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self):
        """åŠ è½½ProtT5å¢å¼ºæ•°æ®"""
        self.logger.info("ğŸ“Š åŠ è½½ProtT5å¢å¼ºæ•°æ®...")
        
        self.train_loader, self.test_loader = create_enhanced_dataloaders(
            dataset=self.args.dataset,
            batch_size=self.enhanced_config['batch_size'],
            use_prot_t5=self.enhanced_config['use_prot_t5'],
            prot_t5_model_path=self.enhanced_config['prot_t5_model_path'],
            device=self.device
        )
        
        self.logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        self.logger.info(f"âœ… æ‰¹æ¬¡å¤§å°: {self.enhanced_config['batch_size']}")
        self.logger.info(f"âœ… ProtT5å¢å¼º: {self.enhanced_config['use_prot_t5']}")
    
    def create_model(self):
        """åˆ›å»ºProtT5å¢å¼ºæ¨¡å‹"""
        self.logger.info("ğŸ—ï¸ åˆ›å»ºProtT5å¢å¼ºæ¨¡å‹...")
        
        self.model = create_enhanced_model(
            num_features_mol=3,
            num_features_pro=25 + 1,  # æ ¹æ®å®é™…ç‰¹å¾ç»´åº¦è°ƒæ•´
            embedding_size=self.enhanced_config['embedding_size'],
            filter_num=self.enhanced_config['filter_num'],
            out_dim=1,
            mask_rate=self.enhanced_config['mask_rate'],
            temperature=self.enhanced_config['temperature'],
            cl_similarity_threshold=self.enhanced_config['cl_similarity_threshold'],
            use_surface=True,
            use_prot_t5=self.enhanced_config['use_prot_t5'],
            prot_t5_fusion_dim=self.enhanced_config['prot_t5_fusion_dim']
        ).to(self.device)
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        feature_info = self.model.get_feature_dimensions()
        self.logger.info(f"âœ… æ¨¡å‹ç‰¹å¾ç»´åº¦: {feature_info}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        self.logger.info("âš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")
        
        # ä¸ºProtT5ç›¸å…³å‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
        prot_t5_params = []
        other_params = []
        
        for name, param in self.model.named_parameters():
            if 'prot_t5_fusion' in name:
                prot_t5_params.append(param)
            else:
                other_params.append(param)
        
        # åˆ†ç»„å‚æ•°ä¼˜åŒ–
        param_groups = [
            {'params': other_params, 'lr': self.enhanced_config['lr']},
            {'params': prot_t5_params, 'lr': self.enhanced_config['lr'] * 0.5}  # ProtT5å‚æ•°ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
        ]
        
        self.optimizer = optim.AdamW(
            param_groups,
            weight_decay=self.enhanced_config['weight_decay']
        )
        
        # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.enhanced_config['max_epochs'], 
            eta_min=1e-6
        )
        
        self.criterion = nn.MSELoss()
        
        self.logger.info(f"âœ… ä¼˜åŒ–å™¨: AdamW (åˆ†ç»„å­¦ä¹ ç‡)")
        self.logger.info(f"âœ… ProtT5å‚æ•°: {len(prot_t5_params)} ä¸ª")
        self.logger.info(f"âœ… å…¶ä»–å‚æ•°: {len(other_params)} ä¸ª")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, data in enumerate(self.train_loader):
            data = data.to(self.device)
            
            self.optimizer.zero_grad()
            
            try:
                # å‰å‘ä¼ æ’­
                pred = self.model(data)
                
                # è®¡ç®—æŸå¤±
                mse_loss = self.criterion(pred.view(-1), data.y.view(-1))
                
                # æ·»åŠ å¯¹æ¯”å­¦ä¹ æŸå¤± (å¦‚æœæ”¯æŒ)
                if hasattr(self.model, 'get_contrastive_loss'):
                    cl_loss = self.model.get_contrastive_loss()
                    total_loss_batch = mse_loss + self.enhanced_config['contrastive_weight'] * cl_loss
                else:
                    total_loss_batch = mse_loss
                
                # åå‘ä¼ æ’­
                total_loss_batch.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.enhanced_config['grad_clip_norm']
                )
                
                self.optimizer.step()
                
                total_loss += total_loss_batch.item()
                num_batches += 1
                
                # å®šæœŸæ‰“å°è¿›åº¦
                if batch_idx % 50 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    prot_t5_lr = self.optimizer.param_groups[1]['lr'] if len(self.optimizer.param_groups) > 1 else current_lr
                    
                    self.logger.info(
                        f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                        f"Loss: {total_loss_batch.item():.4f}, "
                        f"LR: {current_lr:.6f}/{prot_t5_lr:.6f}"
                    )
                    
            except Exception as e:
                self.logger.error(f"è®­ç»ƒæ‰¹æ¬¡å¤±è´¥ (epoch={epoch}, batch={batch_idx}): {e}")
                continue
        
        return total_loss / max(num_batches, 1)
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        pred_list = []
        label_list = []
        
        with torch.no_grad():
            for data in self.test_loader:
                try:
                    data = data.to(self.device)
                    pred = self.model(data)
                    pred_list.append(pred.view(-1).cpu().numpy())
                    label_list.append(data.y.cpu().numpy())
                except Exception as e:
                    self.logger.warning(f"è¯„ä¼°æ‰¹æ¬¡å¤±è´¥: {e}")
                    continue
        
        if not pred_list:
            self.logger.error("è¯„ä¼°å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
            return float('inf'), 0.0, 0.0
        
        predictions = np.concatenate(pred_list)
        labels = np.concatenate(label_list)
        
        mse = np.mean((predictions - labels) ** 2)
        cindex = get_cindex(labels, predictions)
        r2 = get_rm2(labels, predictions)
        
        return mse, cindex, r2
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸš€ å¼€å§‹ProtT5å¢å¼ºè®­ç»ƒ...")
        
        best_mse = float('inf')
        patience_counter = 0
        best_epoch = 0
        
        target_mse = self.enhanced_config['target_mse']
        current_best = self.enhanced_config['current_best_mse']
        
        self.logger.info(f"ğŸ¯ æ€§èƒ½ç›®æ ‡:")
        self.logger.info(f"   - ç›®æ ‡MSE: {target_mse:.4f}")
        self.logger.info(f"   - å½“å‰æœ€ä½³: {current_best:.4f}")
        self.logger.info(f"   - éœ€è¦æ”¹è¿›: {current_best - target_mse:.4f}")
        
        for epoch in range(1, self.enhanced_config['max_epochs'] + 1):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_mse, test_cindex, test_r2 = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•ç»“æœ
            self.logger.info(
                f"Epoch {epoch:4d} | Train Loss: {train_loss:.4f} | "
                f"Test MSE: {test_mse:.4f} | CI: {test_cindex:.4f} | R2: {test_r2:.4f}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_mse < best_mse:
                best_mse = test_mse
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                if self.args.save_model:
                    save_path = f"best_prot_t5_model_mse_{test_mse:.4f}_ci_{test_cindex:.4f}_r2_{test_r2:.4f}.pt"
                    torch.save(self.model.state_dict(), save_path)
                    self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if test_mse <= target_mse:
                    self.logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡MSE! {test_mse:.4f} <= {target_mse:.4f}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¶Šå½“å‰æœ€ä½³
                if test_mse < current_best:
                    improvement = current_best - test_mse
                    progress = (improvement / (current_best - target_mse)) * 100
                    self.logger.info(f"ğŸ† è¶…è¶Šå½“å‰æœ€ä½³! æ”¹è¿›: {improvement:.4f}, è¿›åº¦: {progress:.1f}%")
                
            else:
                patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.enhanced_config['early_stop_patience']:
                self.logger.info(f"â¹ï¸ æ—©åœè§¦å‘")
                break
        
        # è®­ç»ƒå®Œæˆæ€»ç»“
        self.logger.info("=" * 60)
        self.logger.info("ğŸ ProtT5å¢å¼ºè®­ç»ƒå®Œæˆ!")
        self.logger.info(f"ğŸ† æœ€ä½³æ€§èƒ½ (Epoch {best_epoch}): MSE={best_mse:.4f}")
        
        if best_mse <= target_mse:
            self.logger.info("ğŸ‰ å®Œå…¨è¾¾åˆ°ç›®æ ‡æ€§èƒ½!")
        elif best_mse < current_best:
            improvement = current_best - best_mse
            self.logger.info(f"ğŸŠ è¶…è¶ŠåŸºçº¿æ€§èƒ½! æ”¹è¿›: {improvement:.4f}")
        else:
            self.logger.info("ğŸ“ˆ æœªèƒ½è¶…è¶ŠåŸºçº¿ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é…ç½®")
        
        self.logger.info("=" * 60)
        
        return best_mse

def main():
    parser = argparse.ArgumentParser(description='KIBA ProtT5å¢å¼ºè®­ç»ƒ')
    parser.add_argument('--dataset', type=str, default='kiba', help='æ•°æ®é›†åç§°')
    parser.add_argument('--gpu', type=int, default=0, help='GPUè®¾å¤‡å·')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--save_model', action='store_true', default=True, help='ä¿å­˜æœ€ä½³æ¨¡å‹')
    parser.add_argument('--use_prot_t5', action='store_true', default=True, help='ä½¿ç”¨ProtT5åµŒå…¥')
    parser.add_argument('--prot_t5_model_path', type=str, default='/home/lww/prot_t5_model', help='ProtT5æ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®å¯å¤ç°æ€§
    set_reproducible_seeds(args.seed)
    
    print("ğŸ¯ KIBA ProtT5å¢å¼ºè®­ç»ƒ")
    print("=" * 60)
    print("ç›®æ ‡: é€šè¿‡ProtT5è›‹ç™½è´¨åµŒå…¥çªç ´MSE<0.128")
    print("æ–¹æ³•: å¤šæ¨¡æ€ç‰¹å¾èåˆ + å•æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ProtT5EnhancedTrainer(args)
    trainer.load_data()
    trainer.create_model()
    trainer.setup_optimizer()
    
    # å¼€å§‹è®­ç»ƒ
    best_mse = trainer.train()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³MSE: {best_mse:.4f}")
    
    if best_mse < 0.128:
        print("ğŸ† æ­å–œï¼æˆåŠŸè¾¾åˆ°ç›®æ ‡æ€§èƒ½!")
    elif best_mse < 0.1310:
        print("ğŸŠ è¶…è¶Šäº†åŸºçº¿æ€§èƒ½!")
    else:
        print("ğŸ“ˆ æœªèƒ½è¶…è¶ŠåŸºçº¿ï¼Œå»ºè®®è°ƒæ•´é…ç½®")

if __name__ == '__main__':
    main()
