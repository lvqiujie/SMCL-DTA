#!/usr/bin/env python3
"""
ç”ŸæˆKIBAæ•°æ®é›†çš„ProtT5åµŒå…¥
åŸºäºsplit_cold.ipynbä¸­çš„é¢„å¤„ç†ä»£ç 
"""

import os
import pandas as pd
import numpy as np
import torch
import pickle
from transformers import T5EncoderModel, T5Tokenizer
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

def get_T5_model():
    """åŠ è½½ProtT5æ¨¡å‹å’Œtokenizer"""
    model_path = "/home/lww/prot_t5_model"
    
    try:
        print(f"ğŸ”„ åŠ è½½ProtT5æ¨¡å‹ä»: {model_path}")
        model = T5EncoderModel.from_pretrained(model_path, local_files_only=True)
        tokenizer = T5Tokenizer.from_pretrained(model_path, do_lower_case=False, local_files_only=True)
        
        model = model.to(device)
        model.eval()
        
        print("âœ… ProtT5æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ProtT5æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def get_embedding(sequence, model, tokenizer):
    """è·å–å•ä¸ªè›‹ç™½è´¨åºåˆ—çš„åµŒå…¥å‘é‡"""
    # å¤„ç†ç‰¹æ®Šæ°¨åŸºé…¸
    sequence = sequence.replace('U', 'X').replace('Z', 'X').replace('O', 'X')

    # æ£€æµ‹æ˜¯ä½¿ç”¨ProtT5è¿˜æ˜¯ProtBERT
    is_t5 = isinstance(model, T5EncoderModel)

    if is_t5:
        # æ°¨åŸºé…¸ä¹‹é—´åŠ ç©ºæ ¼ (T5éœ€è¦)
        sequence = ' '.join(list(sequence))

    # ç¼–ç åºåˆ—
    inputs = tokenizer(sequence, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # è·å–åµŒå…¥å‘é‡
    with torch.no_grad():
        if is_t5:
            embedding_repr = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])
            # T5è¿”å›çš„æ˜¯last_hidden_state
            emb = embedding_repr.last_hidden_state.mean(dim=1)  # å¯¹æ‰€æœ‰tokenè¿›è¡Œå¹³å‡
        else:
            # ProtBERT
            embedding_repr = model(**inputs)
            # ä½¿ç”¨[CLS]æ ‡è®°çš„è¡¨ç¤º
            emb = embedding_repr.last_hidden_state[:, 0, :]

    return emb.cpu().numpy().squeeze()

def process_protein_set(protein_list, model, tokenizer, batch_size=32):
    """æ‰¹é‡å¤„ç†è›‹ç™½è´¨åºåˆ—å¹¶ç”ŸæˆåµŒå…¥å‘é‡"""
    embeddings = []
    failed_count = 0

    # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡
    for i in range(0, len(protein_list), batch_size):
        batch = protein_list[i:i + batch_size]
        print(f"Processing batch {i // batch_size + 1}/{(len(protein_list) + batch_size - 1) // batch_size}")

        for protein in batch:
            try:
                embedding = get_embedding(protein, model, tokenizer)
                embeddings.append(embedding)
            except Exception as e:
                print(f"âš ï¸ Error processing protein (length={len(protein)}): {e}")
                failed_count += 1
                # æ·»åŠ ä¸€ä¸ªé›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                dim = 1024 if isinstance(model, T5EncoderModel) else 768
                embeddings.append(np.zeros(dim, dtype=np.float32))

    print(f"âœ… å¤„ç†å®Œæˆ: {len(embeddings)} ä¸ªåµŒå…¥, {failed_count} ä¸ªå¤±è´¥")
    return np.array(embeddings)

def run_analysis(df):
    """å¤„ç†è›‹ç™½è´¨åˆ—è¡¨å¹¶è¿›è¡Œèšç±»åˆ†æ"""
    # æå–è›‹ç™½è´¨åºåˆ—é›†åˆ
    drug_set = set()
    protein_set = set()
    for i in range(len(df)):
        drug_set.add(df.loc[i, 'compound_iso_smiles'])
        protein_set.add(df.loc[i, 'target_sequence'])

    print(f"è¯ç‰©æ•°é‡: {len(drug_set)}")
    print(f"è›‹ç™½è´¨æ•°é‡: {len(protein_set)}")

    # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨
    protein_list = list(protein_set)
    
    return protein_list

def create_protein_to_embedding_mapping(df, embeddings, protein_list):
    """åˆ›å»ºè›‹ç™½è´¨åºåˆ—åˆ°åµŒå…¥çš„æ˜ å°„"""
    # åˆ›å»ºåºåˆ—åˆ°åµŒå…¥çš„æ˜ å°„
    protein_to_embedding = {}
    for protein, embedding in zip(protein_list, embeddings):
        protein_to_embedding[protein] = embedding.astype(np.float32)
    
    # åˆ›å»ºç´¢å¼•åˆ°åµŒå…¥çš„æ˜ å°„ (ç”¨äºè®­ç»ƒæ—¶å¿«é€ŸæŸ¥æ‰¾)
    index_to_embedding = {}
    for i, row in df.iterrows():
        protein_seq = row['target_sequence']
        if protein_seq in protein_to_embedding:
            index_to_embedding[i] = protein_to_embedding[protein_seq]
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é›¶å‘é‡
            index_to_embedding[i] = np.zeros(1024, dtype=np.float32)
    
    return protein_to_embedding, index_to_embedding

def main():
    print("ğŸš€ ç”ŸæˆKIBAæ•°æ®é›†çš„ProtT5åµŒå…¥")
    print("=" * 60)
    
    # 1. åŠ è½½KIBAæ•°æ®
    data_path = '/home/lww/learn_project/MGraphDTA-dev/regression/data/kiba/raw/data.csv'
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False
    
    df = pd.read_csv(data_path)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
    
    # 2. æå–è›‹ç™½è´¨åºåˆ—
    print("ğŸ” æå–è›‹ç™½è´¨åºåˆ—...")
    protein_list = run_analysis(df)
    
    # 3. åŠ è½½ProtT5æ¨¡å‹
    model, tokenizer = get_T5_model()
    if model is None:
        print("âŒ æ— æ³•åŠ è½½ProtT5æ¨¡å‹")
        return False
    
    # 4. ç”ŸæˆåµŒå…¥
    print("ğŸ§¬ ç”ŸæˆProtT5åµŒå…¥...")
    embeddings = process_protein_set(protein_list, model, tokenizer, batch_size=16)
    
    # 5. åˆ›å»ºæ˜ å°„
    print("ğŸ—ºï¸ åˆ›å»ºè›‹ç™½è´¨åºåˆ—åˆ°åµŒå…¥çš„æ˜ å°„...")
    protein_to_embedding, index_to_embedding = create_protein_to_embedding_mapping(
        df, embeddings, protein_list
    )
    
    # 6. ä¿å­˜ç»“æœ
    output_dir = "kiba_prot_t5_embeddings"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜åµŒå…¥æ•°ç»„
    embeddings_path = os.path.join(output_dir, "protein_embeddings.npy")
    np.save(embeddings_path, embeddings)
    print(f"âœ… åµŒå…¥æ•°ç»„å·²ä¿å­˜: {embeddings_path}")
    
    # ä¿å­˜è›‹ç™½è´¨åˆ—è¡¨
    protein_list_path = os.path.join(output_dir, "protein_list.pkl")
    with open(protein_list_path, 'wb') as f:
        pickle.dump(protein_list, f)
    print(f"âœ… è›‹ç™½è´¨åˆ—è¡¨å·²ä¿å­˜: {protein_list_path}")
    
    # ä¿å­˜åºåˆ—åˆ°åµŒå…¥çš„æ˜ å°„
    seq_to_emb_path = os.path.join(output_dir, "protein_to_embedding.pkl")
    with open(seq_to_emb_path, 'wb') as f:
        pickle.dump(protein_to_embedding, f)
    print(f"âœ… åºåˆ—æ˜ å°„å·²ä¿å­˜: {seq_to_emb_path}")
    
    # ä¿å­˜ç´¢å¼•åˆ°åµŒå…¥çš„æ˜ å°„
    idx_to_emb_path = os.path.join(output_dir, "index_to_embedding.pkl")
    with open(idx_to_emb_path, 'wb') as f:
        pickle.dump(index_to_embedding, f)
    print(f"âœ… ç´¢å¼•æ˜ å°„å·²ä¿å­˜: {idx_to_emb_path}")
    
    # 7. éªŒè¯ç»“æœ
    print("\nğŸ“Š ç”Ÿæˆç»“æœéªŒè¯:")
    print(f"   - è›‹ç™½è´¨æ•°é‡: {len(protein_list)}")
    print(f"   - åµŒå…¥ç»´åº¦: {embeddings.shape}")
    print(f"   - æ•°æ®è®°å½•æ•°: {len(df)}")
    print(f"   - ç´¢å¼•æ˜ å°„æ•°: {len(index_to_embedding)}")
    
    # æ£€æŸ¥åµŒå…¥è´¨é‡
    non_zero_embeddings = np.sum(np.any(embeddings != 0, axis=1))
    print(f"   - éé›¶åµŒå…¥: {non_zero_embeddings}/{len(embeddings)} ({non_zero_embeddings/len(embeddings)*100:.1f}%)")
    
    print("\nğŸ‰ ProtT5åµŒå…¥ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    return True

if __name__ == '__main__':
    success = main()
    if success:
        print("\nâœ… å¯ä»¥ç»§ç»­ä¿®å¤train_dual_no.pyä¸­çš„ProtT5é›†æˆ")
    else:
        print("\nâŒ åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
