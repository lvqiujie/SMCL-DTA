{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e6cda4-8d02-4a2e-b5b4-9ff3fc56728c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n",
    "\n",
    "\n",
    "def get_T5_model():\n",
    "    local_model_path = \"/home/lww/prot_t5_model\"  # 指向您下载模型的路径\n",
    "    print(\"Loading from local path: {}\".format(local_model_path))\n",
    "    \n",
    "    model = T5EncoderModel.from_pretrained(local_model_path, local_files_only=True)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(local_model_path, do_lower_case=False, local_files_only=True)\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embedding(sequence, model, tokenizer):\n",
    "    \"\"\"获取单个蛋白质序列的嵌入向量\"\"\"\n",
    "    # 处理特殊氨基酸\n",
    "    sequence = sequence.replace('U', 'X').replace('Z', 'X').replace('O', 'X')\n",
    "\n",
    "    # 检测是使用ProtT5还是ProtBERT\n",
    "    is_t5 = isinstance(model, T5EncoderModel)\n",
    "\n",
    "    if is_t5:\n",
    "        # 氨基酸之间加空格 (T5需要)\n",
    "        sequence = ' '.join(list(sequence))\n",
    "\n",
    "    # 编码序列\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 获取嵌入向量\n",
    "    with torch.no_grad():\n",
    "        if is_t5:\n",
    "            embedding_repr = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "            # T5返回的是last_hidden_state\n",
    "            emb = embedding_repr.last_hidden_state.mean(dim=1)  # 对所有token进行平均\n",
    "        else:\n",
    "            # ProtBERT\n",
    "            embedding_repr = model(**inputs)\n",
    "            # 使用[CLS]标记的表示\n",
    "            emb = embedding_repr.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return emb.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "def process_protein_set(protein_list, batch_size=32):\n",
    "    \"\"\"批量处理蛋白质序列并生成嵌入向量\"\"\"\n",
    "    model, tokenizer = get_T5_model()\n",
    "    embeddings = []\n",
    "\n",
    "    # 批量处理以提高效率\n",
    "    for i in range(0, len(protein_list), batch_size):\n",
    "        batch = protein_list[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{len(protein_list) // batch_size + 1}\")\n",
    "\n",
    "        for protein in batch:\n",
    "            try:\n",
    "                embedding = get_embedding(protein, model, tokenizer)\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing protein: {e}\")\n",
    "                # 添加一个空向量作为占位符\n",
    "                # 使用768或1024维度取决于使用的是ProtBERT还是ProtT5\n",
    "                dim = 1024 if isinstance(model, T5EncoderModel) else 768\n",
    "                embeddings.append(np.zeros(dim))\n",
    "\n",
    "    return np.array(embeddings)\n",
    "    \n",
    "def main(protein_list):\n",
    "    \"\"\"处理蛋白质列表并进行聚类分析\"\"\"\n",
    "    # 1. 生成嵌入向量\n",
    "    print(\"生成蛋白质嵌入向量...\")\n",
    "    embeddings = process_protein_set(protein_list)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# 主程序示例，修正集合不可索引的问题\n",
    "def run_analysis(df):\n",
    "    # 提取蛋白质序列集合\n",
    "    drug_set = set()\n",
    "    protein_set = set()\n",
    "    for i in range(len(df)):\n",
    "        drug_set.add(df.loc[i, 'compound_iso_smiles'])\n",
    "        protein_set.add(df.loc[i, 'target_sequence'])\n",
    "\n",
    "    print(f\"药物数量: {len(drug_set)}\")\n",
    "    print(f\"蛋白质数量: {len(protein_set)}\")\n",
    "\n",
    "    # 将集合转换为列表\n",
    "    protein_list = list(protein_set)\n",
    "\n",
    "    # 调用主函数进行分析\n",
    "    embeddings = main(protein_list)\n",
    "    return embeddings, protein_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f414d709-6581-448c-ad1c-ec523882d37d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_embeddings_and_proteins(embeddings, protein_list, output_dir=\"saved_data\"):\n",
    "    \"\"\"\n",
    "    保存嵌入向量和蛋白质序列到文件\n",
    "    \n",
    "    参数:\n",
    "        embeddings: 蛋白质的嵌入向量数组\n",
    "        protein_list: 对应的蛋白质序列列表\n",
    "        output_dir: 保存目录\n",
    "    \"\"\"\n",
    "    # 创建目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存embeddings (numpy array)\n",
    "    np.save(os.path.join(output_dir, \"protein_embeddings.npy\"), embeddings)\n",
    "    \n",
    "    # 保存protein_list (使用pickle)\n",
    "    with open(os.path.join(output_dir, \"protein_list.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(protein_list, f)\n",
    "    \n",
    "    print(f\"数据已保存到 {output_dir} 目录\")\n",
    "    print(f\"- 嵌入向量: {embeddings.shape}\")\n",
    "    print(f\"- 蛋白质序列: {len(protein_list)}\")\n",
    "\n",
    "def load_embeddings_and_proteins(input_dir=\"saved_data\"):\n",
    "    \"\"\"\n",
    "    从文件加载嵌入向量和蛋白质序列\n",
    "    \n",
    "    参数:\n",
    "        input_dir: 保存目录\n",
    "        \n",
    "    返回:\n",
    "        embeddings: 蛋白质的嵌入向量数组\n",
    "        protein_list: 对应的蛋白质序列列表\n",
    "    \"\"\"\n",
    "    # 加载embeddings\n",
    "    embeddings_path = os.path.join(input_dir, \"protein_embeddings.npy\")\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    \n",
    "    # 加载protein_list\n",
    "    protein_list_path = os.path.join(input_dir, \"protein_list.pkl\")\n",
    "    with open(protein_list_path, \"rb\") as f:\n",
    "        protein_list = pickle.load(f)\n",
    "    \n",
    "    print(f\"数据已从 {input_dir} 加载\")\n",
    "    print(f\"- 嵌入向量: {embeddings.shape}\")\n",
    "    print(f\"- 蛋白质序列: {len(protein_list)}\")\n",
    "    \n",
    "    return embeddings, protein_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1dddc07-aa86-4dba-bd85-30ee51f127b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "药物数量: 1767\n",
      "蛋白质数量: 1876\n",
      "生成蛋白质嵌入向量...\n",
      "Loading from local path: /home/lww/prot_t5_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/59\n",
      "Processing batch 2/59\n",
      "Processing batch 3/59\n",
      "Processing batch 4/59\n",
      "Processing batch 5/59\n",
      "Processing batch 6/59\n",
      "Processing batch 7/59\n",
      "Processing batch 8/59\n",
      "Processing batch 9/59\n",
      "Processing batch 10/59\n",
      "Processing batch 11/59\n",
      "Processing batch 12/59\n",
      "Processing batch 13/59\n",
      "Processing batch 14/59\n",
      "Processing batch 15/59\n",
      "Processing batch 16/59\n",
      "Processing batch 17/59\n",
      "Processing batch 18/59\n",
      "Processing batch 19/59\n",
      "Processing batch 20/59\n",
      "Processing batch 21/59\n",
      "Processing batch 22/59\n",
      "Processing batch 23/59\n",
      "Processing batch 24/59\n",
      "Processing batch 25/59\n",
      "Processing batch 26/59\n",
      "Processing batch 27/59\n",
      "Processing batch 28/59\n",
      "Processing batch 29/59\n",
      "Processing batch 30/59\n",
      "Processing batch 31/59\n",
      "Processing batch 32/59\n",
      "Processing batch 33/59\n",
      "Processing batch 34/59\n",
      "Processing batch 35/59\n",
      "Processing batch 36/59\n",
      "Error processing protein: CUDA out of memory. Tried to allocate 20.46 GiB. GPU 7 has a total capacity of 23.64 GiB of which 12.66 GiB is free. Including non-PyTorch memory, this process has 10.97 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 5.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing batch 37/59\n",
      "Processing batch 38/59\n",
      "Processing batch 39/59\n",
      "Processing batch 40/59\n",
      "Error processing protein: CUDA out of memory. Tried to allocate 7.17 GiB. GPU 7 has a total capacity of 23.64 GiB of which 5.49 GiB is free. Including non-PyTorch memory, this process has 18.14 GiB memory in use. Of the allocated memory 12.98 GiB is allocated by PyTorch, and 4.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing batch 41/59\n",
      "Processing batch 42/59\n",
      "Processing batch 43/59\n",
      "Processing batch 44/59\n",
      "Processing batch 45/59\n",
      "Error processing protein: CUDA out of memory. Tried to allocate 7.79 GiB. GPU 7 has a total capacity of 23.64 GiB of which 4.87 GiB is free. Including non-PyTorch memory, this process has 18.76 GiB memory in use. Of the allocated memory 13.70 GiB is allocated by PyTorch, and 4.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing batch 46/59\n",
      "Processing batch 47/59\n",
      "Processing batch 48/59\n",
      "Processing batch 49/59\n",
      "Processing batch 50/59\n",
      "Processing batch 51/59\n",
      "Processing batch 52/59\n",
      "Processing batch 53/59\n",
      "Processing batch 54/59\n",
      "Processing batch 55/59\n",
      "Processing batch 56/59\n",
      "Processing batch 57/59\n",
      "Processing batch 58/59\n",
      "Processing batch 59/59\n",
      "数据已保存到 saved_protein_data 目录\n",
      "- 嵌入向量: (1876, 1024)\n",
      "- 蛋白质序列: 1876\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data/kiba/raw/data.csv'))\n",
    "embeddings, protein_list = run_analysis(df)\n",
    "save_embeddings_and_proteins(embeddings, protein_list, output_dir=\"saved_protein_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b17a88e-88f8-47b5-b37d-f0cceb661bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已从 saved_protein_data 加载\n",
      "- 嵌入向量: (1876, 1024)\n",
      "- 蛋白质序列: 1876\n"
     ]
    }
   ],
   "source": [
    "# 不再需要运行模型，直接从文件加载\n",
    "embeddings, protein_list = load_embeddings_and_proteins(input_dir=\"saved_protein_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b492989e-f621-4447-8666-fa3b19baf617",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def run_balanced_spectral_clustering(embeddings, protein_list, output_dir=\"balanced_spectral_results\"):\n",
    "    \"\"\"\n",
    "    实现先前更均衡的谱聚类结果\n",
    "    \n",
    "    参数:\n",
    "        embeddings: 蛋白质的嵌入向量数组\n",
    "        protein_list: 对应的蛋白质序列列表\n",
    "        output_dir: 结果输出目录\n",
    "    \n",
    "    返回:\n",
    "        labels: 聚类标签\n",
    "        clusters: 聚类结果字典\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 步骤1: 使用UMAP进行降维，这是获得更均衡结果的关键\n",
    "    print(\"执行UMAP降维...\")\n",
    "    import umap\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,       # 较大的邻居数可以捕获更全局的结构\n",
    "        min_dist=0.1,         # 中等的min_dist值\n",
    "        n_components=20,      # 降至较高维度\n",
    "        metric='cosine',      # 使用余弦相似度\n",
    "        random_state=42\n",
    "    )\n",
    "    umap_data = reducer.fit_transform(embeddings)\n",
    "    print(f\"UMAP降维: 从{embeddings.shape[1]}维减至{umap_data.shape[1]}维\")\n",
    "    \n",
    "    # 步骤2: 应用谱聚类，关键参数不同\n",
    "    print(\"执行谱聚类...\")\n",
    "    clustering = SpectralClustering(\n",
    "        n_clusters=25,\n",
    "        affinity='nearest_neighbors',\n",
    "        n_neighbors=15,       # 较大的邻居数\n",
    "        assign_labels='kmeans',  # 使用kmeans分配标签\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    labels = clustering.fit_predict(umap_data)\n",
    "    \n",
    "    # 计算轮廓系数\n",
    "    score = silhouette_score(umap_data, labels)\n",
    "    print(f\"聚类完成，轮廓系数: {score:.4f}\")\n",
    "    \n",
    "    # 将蛋白质分组到各个聚类中\n",
    "    clusters = defaultdict(list)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].append(protein_list[i])\n",
    "    \n",
    "    # 计算每个聚类的大小\n",
    "    cluster_sizes = {label: len(proteins) for label, proteins in clusters.items()}\n",
    "    sorted_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n===== 聚类分析结果 =====\")\n",
    "    print(f\"总蛋白质数量: {len(protein_list)}\")\n",
    "    print(f\"总聚类数量: {len(clusters)}\")\n",
    "    \n",
    "    # 显示每个聚类的大小\n",
    "    for label, size in sorted_clusters:\n",
    "        print(f\"聚类 {label}: {size} 个样本 ({size/len(protein_list)*100:.1f}%)\")\n",
    "    \n",
    "    # 导出每个聚类的蛋白质序列\n",
    "    fasta_dir = os.path.join(output_dir, \"fasta_files\")\n",
    "    os.makedirs(fasta_dir, exist_ok=True)\n",
    "    \n",
    "    for label, proteins in clusters.items():\n",
    "        # 限制大聚类的序列数量\n",
    "        export_proteins = proteins[:min(100, len(proteins))]\n",
    "        \n",
    "        # 写入FASTA文件\n",
    "        with open(os.path.join(fasta_dir, f\"cluster_{label}.fasta\"), \"w\") as f:\n",
    "            for i, seq in enumerate(export_proteins):\n",
    "                f.write(f\">protein_{label}_{i+1}\\n{seq}\\n\")\n",
    "    \n",
    "    return labels, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1245daec-bc9a-4b5f-a9b0-713e00f18f6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行UMAP降维...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lww/anaconda3/envs/bio/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/lww/anaconda3/envs/bio/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP降维: 从1024维减至20维\n",
      "执行谱聚类...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lww/anaconda3/envs/bio/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚类完成，轮廓系数: 0.3281\n",
      "\n",
      "===== 聚类分析结果 =====\n",
      "总蛋白质数量: 1876\n",
      "总聚类数量: 25\n",
      "聚类 8: 228 个样本 (12.2%)\n",
      "聚类 3: 209 个样本 (11.1%)\n",
      "聚类 18: 200 个样本 (10.7%)\n",
      "聚类 1: 144 个样本 (7.7%)\n",
      "聚类 2: 106 个样本 (5.7%)\n",
      "聚类 20: 101 个样本 (5.4%)\n",
      "聚类 0: 95 个样本 (5.1%)\n",
      "聚类 23: 84 个样本 (4.5%)\n",
      "聚类 6: 77 个样本 (4.1%)\n",
      "聚类 22: 70 个样本 (3.7%)\n",
      "聚类 5: 65 个样本 (3.5%)\n",
      "聚类 4: 53 个样本 (2.8%)\n",
      "聚类 16: 51 个样本 (2.7%)\n",
      "聚类 17: 47 个样本 (2.5%)\n",
      "聚类 12: 47 个样本 (2.5%)\n",
      "聚类 11: 46 个样本 (2.5%)\n",
      "聚类 13: 43 个样本 (2.3%)\n",
      "聚类 14: 38 个样本 (2.0%)\n",
      "聚类 24: 33 个样本 (1.8%)\n",
      "聚类 15: 31 个样本 (1.7%)\n",
      "聚类 19: 30 个样本 (1.6%)\n",
      "聚类 21: 29 个样本 (1.5%)\n",
      "聚类 7: 18 个样本 (1.0%)\n",
      "聚类 9: 16 个样本 (0.9%)\n",
      "聚类 10: 15 个样本 (0.8%)\n",
      "聚类 2 包含 106 个蛋白质\n"
     ]
    }
   ],
   "source": [
    "# 假设embeddings和protein_list已加载\n",
    "labels, clusters = run_balanced_spectral_clustering(\n",
    "    embeddings=embeddings,\n",
    "    protein_list=protein_list,\n",
    "    output_dir=\"balanced_spectral_results\"\n",
    ")\n",
    "\n",
    "# 获取特定聚类的蛋白质\n",
    "cluster_id = 2  # 假设我们想查看第2号聚类\n",
    "proteins = clusters[cluster_id]\n",
    "\n",
    "print(f\"聚类 {cluster_id} 包含 {len(proteins)} 个蛋白质\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efa29f06-e11e-4df1-b578-33b199c0c4d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 53,\n",
       " 16: 51,\n",
       " 1: 144,\n",
       " 3: 209,\n",
       " 10: 15,\n",
       " 14: 38,\n",
       " 20: 101,\n",
       " 8: 228,\n",
       " 22: 70,\n",
       " 6: 77,\n",
       " 18: 200,\n",
       " 0: 95,\n",
       " 19: 30,\n",
       " 24: 33,\n",
       " 2: 106,\n",
       " 13: 43,\n",
       " 17: 47,\n",
       " 12: 47,\n",
       " 7: 18,\n",
       " 5: 65,\n",
       " 11: 46,\n",
       " 23: 84,\n",
       " 21: 29,\n",
       " 9: 16,\n",
       " 15: 31}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_sizes = {label: len(proteins) for label, proteins in clusters.items()}\n",
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c108ea5a-19b4-49e2-b66c-36fc39fa5863",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list, cut_tree\n",
    "from rdkit.Chem import AllChem\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "class ScaffoldGenerator(object):\n",
    "    def __init__(self, include_chirality=False):\n",
    "        self.include_chirality = include_chirality\n",
    "\n",
    "    def get_scaffold(self, mol):\n",
    "        return MurckoScaffold.MurckoScaffoldSmiles(\n",
    "            mol=mol, includeChirality=self.include_chirality)\n",
    "\n",
    "\n",
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    return scaffold\n",
    "\n",
    "def search_index(unique_smiles, df, num_class, num_limit):\n",
    "    vec_list = []\n",
    "    for smi in unique_smiles:\n",
    "        m1 = Chem.MolFromSmiles(smi)\n",
    "        fp4 = list(AllChem.GetMorganFingerprintAsBitVect(m1, radius=2, nBits=256))\n",
    "        vec_list.append(fp4)\n",
    "    print(\"drug num\", len(vec_list))\n",
    "    Z = linkage(vec_list, 'average', metric='jaccard')\n",
    "    cluster = cut_tree(Z, num_class).ravel()\n",
    "    stat_dict = {k: v for k, v in sorted(Counter(cluster).items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    num = 0\n",
    "    data_dict = defaultdict(list)\n",
    "    for k,v in stat_dict.items():\n",
    "        pos = np.nonzero(cluster==k)[0]\n",
    "        # print(k, stat_dict[k], len(pos))\n",
    "        smi_idx = []\n",
    "        for idx in pos:\n",
    "            smi_single = df[df[\"compound_iso_smiles\"] == unique_smiles[idx]]\n",
    "            smi_idx.append(smi_single)\n",
    "        df_tmp = pd.concat(smi_idx)\n",
    "        num += len(df_tmp)\n",
    "        data_dict[k] = df_tmp\n",
    "    print(\"@@@@@@@@@@@\", len(data_dict.keys()), num)\n",
    "\n",
    "    num = 0\n",
    "    all_keys = list(data_dict.keys())\n",
    "    class_num = -1\n",
    "    meat_class = {}\n",
    "    for k,v in data_dict.items():\n",
    "        if len(v) > num_limit:\n",
    "            class_num += 1\n",
    "            meat_class[class_num] = v\n",
    "            num += len(v)\n",
    "            all_keys.remove(k)\n",
    "    random.shuffle(all_keys)\n",
    "\n",
    "    smi_idx = []\n",
    "    smi_idx_num = 0\n",
    "    for i,k in enumerate(all_keys):\n",
    "        # print(i, len(data_dict[k]))\n",
    "        if smi_idx_num < num_limit:\n",
    "            smi_idx.append(data_dict[k])\n",
    "            smi_idx_num += len(data_dict[k])\n",
    "        else:\n",
    "            class_num += 1\n",
    "            meat_class[class_num] = pd.concat(smi_idx)\n",
    "            num += len(meat_class[class_num])\n",
    "\n",
    "            smi_idx = []\n",
    "            smi_idx_num = 0\n",
    "            smi_idx.append(data_dict[k])\n",
    "            smi_idx_num += len(data_dict[k])\n",
    "        if i == len(all_keys) -1:\n",
    "            class_num += 1\n",
    "            meat_class[class_num] = pd.concat(smi_idx)\n",
    "            num += len(meat_class[class_num])\n",
    "\n",
    "    print(class_num, len(meat_class[class_num]),num)\n",
    "\n",
    "    if len(meat_class[class_num]) < 10:\n",
    "        meat_class.pop(class_num)\n",
    "\n",
    "    num = 0\n",
    "    for k,v in meat_class.items():\n",
    "        num += len(v)\n",
    "    print(num)\n",
    "    return meat_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aaf31df-23cb-494e-aeee-9bf839c3faaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7786\n",
      "1767\n",
      "1876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:35:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/raw/data.csv'))\n",
    "scaffolds = {}\n",
    "protberts = {}\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "drug_set = set()\n",
    "protein_set = set()\n",
    "for i in range(len(df)):\n",
    "    drug_set.add(df.loc[i, 'compound_iso_smiles'])\n",
    "    protein_set.add(df.loc[i, 'target_sequence'])  # smile\n",
    "\n",
    "print(len(drug_set))\n",
    "print(len(protein_set))\n",
    "\n",
    "for d in drug_set:\n",
    "    try:\n",
    "        scaffold = generate_scaffold(d)\n",
    "        if scaffolds.__contains__(scaffold):\n",
    "            scaffolds[scaffold] = scaffolds[scaffold] + 1\n",
    "        else:\n",
    "            scaffolds[scaffold] = 1\n",
    "    except:\n",
    "        print(\"error\", d)\n",
    "        # df.drop(index=i, inplace=True)\n",
    "        continue\n",
    "\n",
    "cluster_sizes = {label: len(proteins) for label, proteins in clusters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8be3e8b-10c3-4202-84e7-3595f9c8c235",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:35:42] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "smile_scafold = {}\n",
    "for d in drug_set:\n",
    "    smile_scafold[d] = generate_scaffold(d)\n",
    "\n",
    "protein_cluster = {}\n",
    "for i, protein in enumerate(protein_list):\n",
    "    protein_cluster[protein] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aff13bdd-1cf7-41e4-997e-2274ee91e6f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "0.8653084323712507\n",
      "25\n",
      "0.8768656716417911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1976062/2510592015.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  train_scaffold = random.sample(all_key, round(len(all_key) * 0.80))\n"
     ]
    }
   ],
   "source": [
    "all_key = scaffolds.keys()\n",
    "print(len(all_key))  # all_key_drugs\n",
    "train_scaffold = random.sample(all_key, round(len(all_key) * 0.80))\n",
    "total_count1 = sum(scaffolds[s] for s in train_scaffold)\n",
    "print(total_count1/len(drug_set))\n",
    "\n",
    "\n",
    "# 蛋白质聚类划分等效代码\n",
    "all_clusters = list(clusters.keys())  # 获取所有聚类标签\n",
    "print(len(all_clusters))  # 打印聚类数量\n",
    "\n",
    "# 随机选择88%的聚类作为训练集\n",
    "train_clusters = random.sample(all_clusters, round(len(all_clusters) * 0.80))\n",
    "\n",
    "# 计算训练集中的蛋白质数量\n",
    "total_count_proteins = sum(len(clusters[c]) for c in train_clusters)\n",
    "\n",
    "# 打印训练集蛋白质占总蛋白质的比例\n",
    "print(total_count_proteins/len(protein_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7ef1d0d-3f45-4f36-841c-ad516c4a0045",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "0.8834182229767968\n",
      "25\n",
      "0.8406183368869936\n",
      "5752 1901 133 7786\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # 1. 药物支架划分\n",
    "    all_key = list(scaffolds.keys())\n",
    "    print(len(all_key))  # all_key_drugs\n",
    "    train_scaffold = random.sample(all_key, round(len(all_key) * 0.80))\n",
    "    total_count1 = sum(scaffolds[s] for s in train_scaffold)\n",
    "    print(total_count1 / len(drug_set))\n",
    "\n",
    "    # 2. 蛋白质聚类划分\n",
    "    all_clusters = list(clusters.keys())  # 获取所有聚类标签\n",
    "    print(len(all_clusters))  # 打印聚类数量\n",
    "\n",
    "    train_clusters = random.sample(all_clusters, round(len(all_clusters) * 0.80))\n",
    "    total_count_proteins = sum(len(clusters[c]) for c in train_clusters)\n",
    "    print(total_count_proteins / len(protein_list))\n",
    "\n",
    "    # 3. 根据支架和聚类进行数据划分\n",
    "    train_idx = []\n",
    "    test1_idx = []\n",
    "    test2_idx = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        smi = smile_scafold[df.loc[i, 'compound_iso_smiles']]\n",
    "        prot = protein_cluster[df.loc[i, 'target_sequence']]\n",
    "\n",
    "        if smi in train_scaffold and prot in train_clusters:\n",
    "            train_idx.append(i)  # 都在，加入训练集\n",
    "        elif smi not in train_scaffold and prot not in train_clusters:\n",
    "            test2_idx.append(i)  # 都不在，加入 test2\n",
    "        else:\n",
    "            test1_idx.append(i)  # 一个在一个不在，加入 test1\n",
    "\n",
    "    print(len(train_idx), len(test1_idx), len(test2_idx), len(train_idx) + len(test1_idx) + len(test2_idx))\n",
    "\n",
    "    # 4. 如果训练集满足长度要求，则保存数据并退出循环\n",
    "    if len(train_idx) > (len(train_idx) + len(test1_idx) + len(test2_idx))*0.8):\n",
    "        df_old = df.loc[train_idx].reset_index(drop=True)\n",
    "        df_old.to_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/train_val.csv')\n",
    "\n",
    "        df_test1 = df.loc[test1_idx].reset_index(drop=True)\n",
    "        df_test1.to_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/test1.csv')\n",
    "\n",
    "        df_test2 = df.loc[test2_idx].reset_index(drop=True)\n",
    "        df_test2.to_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/test2.csv')\n",
    "\n",
    "        print(\"done\")\n",
    "        break  # 满足条件，退出循环\n",
    "    else:\n",
    "        print(\"训练集样本不足，重新执行划分操作...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cf46c-0b28-4a16-b7ca-66ab98e474cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/train_val.csv')\n",
    "unique_smi = df_train[\"compound_iso_smiles\"].unique()\n",
    "num_classes = len(unique_smi) // 10\n",
    "num_limit = len(df_train) // 50\n",
    "meat_class = search_index(unique_smi, df_train, num_classes, num_limit)\n",
    "print(len(meat_class.keys()))\n",
    "meta_train = {}\n",
    "meta_train_num = 0\n",
    "meta_train_k_num = 0\n",
    "meta_val = {}\n",
    "meta_val_num = 0\n",
    "meta_val_k_num = 0\n",
    "meta_keys = list(meat_class.keys())\n",
    "random.shuffle(meta_keys)\n",
    "for k in meta_keys:\n",
    "    if len(meta_train.keys()) < len(meta_keys) *0.8:\n",
    "        meta_train[k] = meat_class[k]\n",
    "        meta_train_num += len(meat_class[k])\n",
    "        meta_train_k_num += 1\n",
    "    else:\n",
    "        meta_val_k_num +=1\n",
    "        meta_val[k] = meat_class[k]\n",
    "        meta_val_num += len(meat_class[k])\n",
    "\n",
    "joblib.dump(meta_train, \"/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/meta_train.pkl\")\n",
    "joblib.dump(meta_val, \"/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/meta_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4538121-3416-4909-b820-43fb21551592",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5254\n",
      "1193\n"
     ]
    }
   ],
   "source": [
    "meta_train = joblib.load(\"/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/meta_train.pkl\")\n",
    "meta_val = joblib.load(\"/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/meta_val.pkl\")\n",
    "train_pd = []\n",
    "for k,v in meta_train.items():\n",
    "    train_pd.append(v)\n",
    "df_tmp = pd.concat(train_pd)\n",
    "df_tmp = df_tmp.reset_index(drop=True)\n",
    "df_tmp.to_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/train.csv')\n",
    "print(len(df_tmp))\n",
    "\n",
    "test_pd = []\n",
    "for k, v in meta_val.items():\n",
    "    test_pd.append(v)\n",
    "df_tmp = pd.concat(test_pd)\n",
    "df_tmp = df_tmp.reset_index(drop=True)\n",
    "df_tmp.to_csv('/home/lww/learn_project/MGraphDTA-dev/classification/data/celegans/val.csv')\n",
    "\n",
    "print(len(df_tmp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
