#!/usr/bin/env python3
"""
KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒè„šæœ¬
æ•´åˆæ‰€æœ‰æœ€ä½³å®è·µåˆ°ä¸€ä¸ªç®€æ´çš„è®­ç»ƒæµç¨‹ä¸­
ç›®æ ‡: ç›´æ¥è¾¾åˆ°MSE~0.1310, CI~0.8886, R2~0.8035çš„æ€§èƒ½

åŸºäºå¤šé˜¶æ®µä¼˜åŒ–å‘ç°çš„æœ€ä½³é…ç½®:
- è¡¨é¢ç‰¹å¾: use_surface=True, use_masif=True
- å¯¹æ¯”å­¦ä¹ : contrastive_weight=0.03
- ä¼˜åŒ–å™¨: AdamW with cosine scheduling
- è®­ç»ƒç­–ç•¥: é•¿æœŸè®­ç»ƒ + æ—©åœ + æ¢¯åº¦è£å‰ª
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import argparse
import logging
from datetime import datetime
from torch_geometric.data import DataLoader

from src.model_0428_16_dual import MGraphDTA
from src.dataset import GNNDataset
from src.metrics import get_cindex, get_rm2

def set_reproducible_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å®Œå…¨å¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

class OptimizedTrainer:
    """ä¼˜åŒ–çš„å•æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # æœ€ä½³è¶…å‚æ•°é…ç½® (åŸºäºå¤šé˜¶æ®µä¼˜åŒ–å‘ç°)
        self.best_config = {
            'embedding_size': 128,
            'filter_num': 32,
            'mask_rate': 0.05,
            'temperature': 0.1,
            'cl_similarity_threshold': 0.5,
            'contrastive_weight': 0.03,  # å…³é”®å‘ç°
            'lr': 3e-4,                  # æœ€ä½³å­¦ä¹ ç‡
            'weight_decay': 1e-4,
            'batch_size': 512,           # å‡å°æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜
            'max_epochs': 3000,          # é•¿æœŸè®­ç»ƒçš„é‡è¦æ€§
            'early_stop_patience': 400,
            'grad_clip_norm': 1.0
        }
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–KIBAå•æ¨¡å‹è®­ç»ƒå™¨")
        self.logger.info(f"ğŸ“Š æœ€ä½³é…ç½®: {self.best_config}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = f"logs/single_model_{timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{log_dir}/training.log"),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self):
        """åŠ è½½KIBAæ•°æ®é›†"""
        self.logger.info("ğŸ“Š åŠ è½½KIBAæ•°æ®é›†...")
        
        fpath = os.path.join('/home/lww/learn_project/MGraphDTA-dev/regression/data', self.args.dataset)
        
        # ä½¿ç”¨æœ€ä½³æ•°æ®é…ç½®
        train_set = GNNDataset(fpath, types='train', use_surface=True, use_masif=True)
        test1_set = GNNDataset(fpath, types='test1', use_surface=True, use_masif=True)
        
        self.train_loader = DataLoader(
            train_set, 
            batch_size=self.best_config['batch_size'], 
            shuffle=True, 
            num_workers=8
        )
        self.test_loader = DataLoader(
            test1_set, 
            batch_size=self.best_config['batch_size'], 
            shuffle=False, 
            num_workers=8
        )
        
        self.logger.info(f"âœ… è®­ç»ƒé›†: {len(train_set)} æ ·æœ¬")
        self.logger.info(f"âœ… æµ‹è¯•é›†: {len(test1_set)} æ ·æœ¬")
        self.logger.info(f"âœ… æ‰¹æ¬¡å¤§å°: {self.best_config['batch_size']}")
        self.logger.info(f"âœ… è¡¨é¢ç‰¹å¾: å¯ç”¨ (use_surface=True, use_masif=True)")
    
    def create_model(self):
        """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
        self.logger.info("ğŸ—ï¸ åˆ›å»ºMGraphDTAæ¨¡å‹...")
        
        self.model = MGraphDTA(
            3, 25 + 1,
            embedding_size=self.best_config['embedding_size'],
            filter_num=self.best_config['filter_num'],
            out_dim=1,
            mask_rate=self.best_config['mask_rate'],
            temperature=self.best_config['temperature'],
            disable_masking=False,
            cl_mode='regression',
            cl_similarity_threshold=self.best_config['cl_similarity_threshold'],
            use_surface=True  # å…³é”®ç‰¹å¾
        ).to(self.device)
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        self.logger.info(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,} æ€»è®¡, {trainable_params:,} å¯è®­ç»ƒ")
        self.logger.info(f"âœ… å¯¹æ¯”å­¦ä¹ æƒé‡: {self.best_config['contrastive_weight']}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        self.logger.info("âš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")
        
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ (æ¯”Adamæ›´å¥½)
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.best_config['lr'],
            weight_decay=self.best_config['weight_decay']
        )
        
        # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.best_config['max_epochs'], 
            eta_min=1e-6
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        self.logger.info(f"âœ… ä¼˜åŒ–å™¨: AdamW (lr={self.best_config['lr']}, wd={self.best_config['weight_decay']})")
        self.logger.info(f"âœ… è°ƒåº¦å™¨: CosineAnnealingLR")
        self.logger.info(f"âœ… æŸå¤±å‡½æ•°: MSELoss")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, data in enumerate(self.train_loader):
            data = data.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            pred = self.model(data)
            
            # è®¡ç®—æŸå¤± (åŒ…å«å¯¹æ¯”å­¦ä¹ )
            mse_loss = self.criterion(pred.view(-1), data.y.view(-1))
            
            # è·å–å¯¹æ¯”å­¦ä¹ æŸå¤±
            if hasattr(self.model, 'get_contrastive_loss'):
                cl_loss = self.model.get_contrastive_loss()
                total_loss_batch = mse_loss + self.best_config['contrastive_weight'] * cl_loss
            else:
                total_loss_batch = mse_loss
            
            # åå‘ä¼ æ’­
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.best_config['grad_clip_norm']
            )
            
            self.optimizer.step()
            
            total_loss += total_loss_batch.item()
            num_batches += 1
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                self.logger.info(
                    f"Epoch {epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                    f"Loss: {total_loss_batch.item():.4f}, LR: {current_lr:.6f}"
                )
        
        return total_loss / num_batches
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        pred_list = []
        label_list = []
        
        with torch.no_grad():
            for data in self.test_loader:
                data = data.to(self.device)
                pred = self.model(data)
                pred_list.append(pred.view(-1).cpu().numpy())
                label_list.append(data.y.cpu().numpy())
        
        predictions = np.concatenate(pred_list)
        labels = np.concatenate(label_list)
        
        mse = np.mean((predictions - labels) ** 2)
        cindex = get_cindex(labels, predictions)
        r2 = get_rm2(labels, predictions)
        
        return mse, cindex, r2
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        best_mse = float('inf')
        patience_counter = 0
        best_epoch = 0
        
        # ç›®æ ‡æ€§èƒ½ (åŸºäºå¤šé˜¶æ®µä¼˜åŒ–ç»“æœ)
        target_mse = 0.1310
        target_ci = 0.8886
        target_r2 = 0.8035
        
        self.logger.info(f"ğŸ¯ ç›®æ ‡æ€§èƒ½: MSE={target_mse:.4f}, CI={target_ci:.4f}, R2={target_r2:.4f}")
        
        for epoch in range(1, self.best_config['max_epochs'] + 1):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_mse, test_cindex, test_r2 = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•ç»“æœ
            self.logger.info(
                f"Epoch {epoch:4d} | Train Loss: {train_loss:.4f} | "
                f"Test MSE: {test_mse:.4f} | CI: {test_cindex:.4f} | R2: {test_r2:.4f} | "
                f"LR: {current_lr:.6f}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_mse < best_mse:
                best_mse = test_mse
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                if self.args.save_model:
                    save_path = f"best_single_model_mse_{test_mse:.4f}_ci_{test_cindex:.4f}_r2_{test_r2:.4f}.pt"
                    torch.save(self.model.state_dict(), save_path)
                    self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
                
                self.logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ€§èƒ½! MSE: {test_mse:.4f}")
            else:
                patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= self.best_config['early_stop_patience']:
                self.logger.info(f"â¹ï¸ æ—©åœè§¦å‘ (patience={self.best_config['early_stop_patience']})")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ€§èƒ½
            if test_mse <= target_mse * 1.01:  # 1%å®¹å·®
                self.logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡MSEæ€§èƒ½! {test_mse:.4f} <= {target_mse:.4f}")
        
        # è®­ç»ƒå®Œæˆæ€»ç»“
        self.logger.info("=" * 60)
        self.logger.info("ğŸ è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"ğŸ† æœ€ä½³æ€§èƒ½ (Epoch {best_epoch}): MSE={best_mse:.4f}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡è¾¾æˆåº¦: {(target_mse/best_mse)*100:.1f}%")
        self.logger.info("=" * 60)
        
        return best_mse

def main():
    parser = argparse.ArgumentParser(description='KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒ')
    parser.add_argument('--dataset', type=str, default='kiba', help='æ•°æ®é›†åç§°')
    parser.add_argument('--gpu', type=int, default=0, help='GPUè®¾å¤‡å·')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--save_model', action='store_true', default=True, help='ä¿å­˜æœ€ä½³æ¨¡å‹')
    
    args = parser.parse_args()
    
    # è®¾ç½®å¯å¤ç°æ€§
    set_reproducible_seeds(args.seed)
    
    print("ğŸ¯ KIBAå•æ¨¡å‹ç»Ÿä¸€è®­ç»ƒ")
    print("=" * 60)
    print("ç›®æ ‡: é€šè¿‡å•ä¸€è®­ç»ƒè¿‡ç¨‹ç›´æ¥è¾¾åˆ°æœ€ä½³æ€§èƒ½")
    print("é…ç½®: æ•´åˆæ‰€æœ‰å¤šé˜¶æ®µä¼˜åŒ–å‘ç°çš„æœ€ä½³å®è·µ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = OptimizedTrainer(args)
    trainer.load_data()
    trainer.create_model()
    trainer.setup_optimizer()
    
    # å¼€å§‹è®­ç»ƒ
    best_mse = trainer.train()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³MSE: {best_mse:.4f}")
    print("ğŸ“‹ æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ç›´æ¥ç”¨äºè®ºæ–‡ç»“æœå¤ç°")

if __name__ == '__main__':
    main()
